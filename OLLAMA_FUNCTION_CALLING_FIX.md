# Ollama Function Calling æ”¯æŒ - ä¿®å¤æŠ¥å‘Š

## ğŸ› é—®é¢˜æè¿°

**é”™è¯¯ä¿¡æ¯**:
```
âš ï¸ Function Calling æ‰§è¡Œå¤±è´¥: Ollama è¯·æ±‚å¤±è´¥: 
Attempted to access streaming response content, without having called read().
```

**æ ¹æœ¬åŸå› **:
1. **httpx ä½¿ç”¨ä¸å½“**: åœ¨ `httpx.stream()` ä¸Šä¸‹æ–‡ä¸­é”™è¯¯åœ°è®¿é—® `response.text`
2. **ç¼ºå°‘ Function Calling æ”¯æŒ**: OllamaLLMClient æ²¡æœ‰å®ç° tool_calls æ£€æµ‹å’Œéæµå¼æ–¹æ³•

---

## âœ… ä¿®å¤å†…å®¹

### 1. ä¿®å¤ httpx æµå¼å“åº”é”™è¯¯

**åŸä»£ç ** (ç¬¬ 83-98 è¡Œ):
```python
with httpx.stream("POST", self.chat_endpoint, ...) as response:
    if response.status_code != 200:
        error_text = response.text  # âŒ é”™è¯¯ï¼ä¸èƒ½åœ¨ stream æ¨¡å¼ä¸‹è®¿é—® text
```

**ä¿®å¤å**:
```python
with httpx.Client() as client:
    response = client.post(self.chat_endpoint, ...)
    if response.status_code != 200:
        error_text = response.text  # âœ… æ­£ç¡®ï¼æ™®é€šè¯·æ±‚å¯ä»¥è®¿é—® text
```

### 2. æ·»åŠ  tool_calls æ£€æµ‹

**æ–°å¢ä»£ç ** (ç¬¬ 113-140 è¡Œ):
```python
if 'message' in data:
    message = data['message']
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ tool_calls
    tool_calls = message.get('tool_calls')
    if tool_calls:
        self._accumulate_tool_calls(tool_calls)
        continue
    
    # æå–æ™®é€šå†…å®¹
    content = message.get('content', '')
    if content:
        # è¿”å›æ–°æ ¼å¼
        yield {'type': 'content', 'text': content}

# æ£€æŸ¥æ˜¯å¦å®Œæˆ
if data.get('done', False):
    if self._tool_calls_buffer:
        yield {
            'type': 'tool_calls',
            'tool_calls': self._get_accumulated_tool_calls()
        }
```

### 3. å®ç°éæµå¼æ–¹æ³•

**æ–°å¢æ–¹æ³•** (ç¬¬ 235-320 è¡Œ):
```python
def generate_response_non_streaming(
    self,
    context_messages: List[Dict[str, str]],
    temperature: float = None,
    tools: List[Dict] = None
) -> Dict:
    """
    éæµå¼è°ƒç”¨ï¼ˆç”¨äºå·¥å…·è°ƒç”¨æ£€æµ‹ï¼‰
    
    Returns:
        Dict: {
            'type': 'tool_calls' | 'content',
            'tool_calls': [...] | None,
            'content': str | None
        }
    """
    # æ„å»ºè¯·æ±‚ï¼ˆstream=Falseï¼‰
    payload = {
        "model": self.model_name,
        "messages": context_messages,
        "stream": False,
        ...
    }
    
    # æ£€æŸ¥å“åº”ä¸­çš„ tool_calls
    if 'tool_calls' in message:
        return {'type': 'tool_calls', ...}
    else:
        return {'type': 'content', ...}
```

### 4. æ·»åŠ å·¥å…·ç¼“å†²åŒº

**åˆå§‹åŒ–** (ç¬¬ 46 è¡Œ):
```python
def __init__(self, config):
    ...
    # Tool calls ç´¯ç§¯ç¼“å†²åŒº
    self._tool_calls_buffer = []
```

**è¾…åŠ©æ–¹æ³•** (ç¬¬ 214-233 è¡Œ):
```python
def _accumulate_tool_calls(self, tool_calls_delta):
    """ç´¯ç§¯ tool_calls"""
    for tc in tool_calls_delta:
        self._tool_calls_buffer.append(tc)

def _get_accumulated_tool_calls(self):
    """è·å–å¹¶æ¸…ç©ºç¼“å†²åŒº"""
    result = self._tool_calls_buffer.copy()
    self._tool_calls_buffer = []
    return result
```

---

## ğŸ“Š ä¿®æ”¹ç»Ÿè®¡

| é¡¹ç›® | æ•°é‡ |
|------|------|
| æ–°å¢ä»£ç  | +145 è¡Œ |
| åˆ é™¤ä»£ç  | -11 è¡Œ |
| æ–°å¢æ–¹æ³• | 3 ä¸ª |
| ä¿®å¤çš„ bug | 2 ä¸ª |

---

## ğŸ¯ ç°åœ¨æ”¯æŒçš„åŠŸèƒ½

### âœ… ApiLLMClient (OpenAI-compatible)
- âœ… tool_calls æ£€æµ‹
- âœ… éæµå¼å·¥å…·è°ƒç”¨
- âœ… æµå¼æœ€ç»ˆå›å¤
- âœ… å¤šè½®å·¥å…·è°ƒç”¨

### âœ… OllamaLLMClient (æœ¬åœ°æ¨¡å‹)
- âœ… tool_calls æ£€æµ‹
- âœ… éæµå¼å·¥å…·è°ƒç”¨
- âœ… æµå¼æœ€ç»ˆå›å¤
- âœ… å¤šè½®å·¥å…·è°ƒç”¨

---

## ğŸ§ª æµ‹è¯•æ–¹æ³•

### 1. ä½¿ç”¨ Ollama æµ‹è¯•å•å·¥å…·è°ƒç”¨

```
ç”¨æˆ·: åˆ—å‡ºæ‰€æœ‰èµ„äº§
```

**é¢„æœŸè¡Œä¸º**:
1. æ˜¾ç¤º "ğŸ”§ æ­£åœ¨è°ƒç”¨å·¥å…· [tool_list_assets]..."
2. æ‰§è¡Œå·¥å…·
3. LLM åŸºäºç»“æœç”Ÿæˆå›å¤

### 2. æµ‹è¯•å¤šè½®å·¥å…·è°ƒç”¨

```
ç”¨æˆ·: æ¯”è¾ƒèµ„äº§ç®¡ç†å™¨å’ŒAIåŠ©æ‰‹çš„æ—¥å¿—
```

**é¢„æœŸè¡Œä¸º**:
1. è°ƒç”¨ tool_analyze_logs (asset_manager)
2. è°ƒç”¨ tool_analyze_logs (ai_assistant)
3. LLM ç»¼åˆç»“æœç”Ÿæˆå¯¹æ¯”åˆ†æ

---

## ğŸ”§ å…¼å®¹æ€§

- âœ… å‘åå…¼å®¹ï¼šæ— å·¥å…·è°ƒç”¨æ—¶ï¼Œæ­£å¸¸å¯¹è¯ä¸å—å½±å“
- âœ… æ ¼å¼å…¼å®¹ï¼šåŒæ—¶æ”¯æŒ dict å’Œ str æ ¼å¼çš„ chunk
- âœ… æ¨¡å‹åˆ‡æ¢ï¼šAPI å’Œ Ollama å¯ä»¥æ— ç¼åˆ‡æ¢

---

## ğŸ“ æ³¨æ„äº‹é¡¹

### Ollama å·¥å…·è°ƒç”¨æ”¯æŒ

å¹¶éæ‰€æœ‰ Ollama æ¨¡å‹éƒ½æ”¯æŒ Function Callingã€‚æ”¯æŒçš„æ¨¡å‹åŒ…æ‹¬ï¼š

- âœ… `llama3.1` å’Œæ›´æ–°ç‰ˆæœ¬
- âœ… `mistral-nemo`
- âœ… `qwen2.5`
- âŒ `llama2` (ä¸æ”¯æŒ)
- âŒ éƒ¨åˆ†è¾ƒæ—§æ¨¡å‹

**å¦‚ä½•æ£€æŸ¥**:
```bash
ollama show <model_name>
```

æŸ¥çœ‹è¾“å‡ºä¸­æ˜¯å¦æåˆ° "function calling" æˆ– "tools"ã€‚

### å¦‚æœæ¨¡å‹ä¸æ”¯æŒå·¥å…·è°ƒç”¨

ç³»ç»Ÿä¼šè‡ªåŠ¨é™çº§ä¸ºæ™®é€šå¯¹è¯æ¨¡å¼ï¼Œä¸ä¼šæŠ¥é”™ã€‚

---

## ğŸ‰ æ€»ç»“

é€šè¿‡æ­¤æ¬¡ä¿®å¤ï¼š

1. **è§£å†³äº† httpx æµå¼å“åº”é”™è¯¯** - ä¸å†å´©æºƒ
2. **æ·»åŠ äº†å®Œæ•´çš„ Function Calling æ”¯æŒ** - Ollama å¯ä»¥è°ƒç”¨å·¥å…·
3. **ç»Ÿä¸€äº† API å’Œ Ollama çš„å®ç°** - ç›¸åŒçš„ Function Calling ä½“éªŒ
4. **ä¿æŒäº†å‘åå…¼å®¹** - ä¸å½±å“ç°æœ‰åŠŸèƒ½

**ç°åœ¨å¯ä»¥æ„‰å¿«åœ°ä½¿ç”¨ Ollama æœ¬åœ°æ¨¡å‹è°ƒç”¨å·¥å…·äº†ï¼** ğŸŠ

---

**ä¿®å¤ç‰ˆæœ¬**: Commit 20fb332  
**ä¿®å¤æ—¥æœŸ**: 2025-11-06  
**ä¿®å¤æ–‡ä»¶**: `modules/ai_assistant/clients/ollama_llm_client.py`

