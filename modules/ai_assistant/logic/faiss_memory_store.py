"""
FAISS å‘é‡å­˜å‚¨ç®¡ç†å™¨

åŠŸèƒ½ï¼š
- ä½¿ç”¨ FAISS ä½œä¸ºä¸»å­˜å‚¨å¼•æ“
- é«˜æ•ˆçš„å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢
- æ”¯æŒå…ƒæ•°æ®å­˜å‚¨å’Œè¿‡æ»¤
- è‡ªåŠ¨æŒä¹…åŒ–åˆ°ç£ç›˜
- JSON å¤‡ä»½æœºåˆ¶

ä½œè€…ï¼šAI Assistant
æ—¥æœŸï¼š2025-11-06
"""

import json
import pickle
import numpy as np
from pathlib import Path
from typing import List, Dict, Optional, Any, Tuple
from datetime import datetime
import logging


def _get_logger():
    """è·å–æ—¥å¿—è®°å½•å™¨"""
    return logging.getLogger("ue_toolkit.modules.ai_assistant.logic.faiss_memory_store")


class FaissMemoryStore:
    """FAISS å‘é‡å­˜å‚¨ç®¡ç†å™¨
    
    æ¶æ„è®¾è®¡ï¼š
    1. FAISS ç´¢å¼•ï¼šå­˜å‚¨å‘é‡ï¼Œæ”¯æŒé«˜æ•ˆæ£€ç´¢
    2. å…ƒæ•°æ®æ˜ å°„ï¼šID -> {content, metadata, timestamp, importance}
    3. ç£ç›˜æŒä¹…åŒ–ï¼šå®šæœŸä¿å­˜åˆ°ç£ç›˜
    4. JSON å¤‡ä»½ï¼šç¾éš¾æ¢å¤
    """
    
    def __init__(self, storage_dir: Path, vector_dim: int = 512, user_id: str = "default"):
        """åˆå§‹åŒ– FAISS å­˜å‚¨
        
        Args:
            storage_dir: å­˜å‚¨ç›®å½•
            vector_dim: å‘é‡ç»´åº¦ï¼ˆé»˜è®¤ 512ï¼ŒåŒ¹é… bge-small-zh-v1.5ï¼‰
            user_id: ç”¨æˆ· ID
        """
        self.logger = _get_logger()
        self.storage_dir = Path(storage_dir)
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        
        self.user_id = user_id
        self.vector_dim = vector_dim
        
        # æ–‡ä»¶è·¯å¾„
        self.index_file = self.storage_dir / f"{user_id}_faiss.index"
        self.metadata_file = self.storage_dir / f"{user_id}_metadata.pkl"
        self.backup_file = self.storage_dir / f"{user_id}_backup.json"
        
        # åˆå§‹åŒ– FAISS ç´¢å¼•
        self.index = None
        self.id_to_metadata = {}  # {memory_id: {content, metadata, timestamp, importance}}
        self.next_id = 0
        
        # åŠ è½½ç°æœ‰æ•°æ®
        self._load_from_disk()
        
        self.logger.info(f"âœ… FAISS å­˜å‚¨åˆå§‹åŒ–å®Œæˆï¼ˆç”¨æˆ·: {user_id}, å‘é‡ç»´åº¦: {vector_dim}, å·²æœ‰è®°å¿†: {self.count()}ï¼‰")
    
    def _load_from_disk(self):
        """ä»ç£ç›˜åŠ è½½ç´¢å¼•å’Œå…ƒæ•°æ®"""
        try:
            # å°è¯•åŠ è½½ FAISS ç´¢å¼•
            if self.index_file.exists():
                import faiss
                self.index = faiss.read_index(str(self.index_file))
                self.logger.info(f"ğŸ“‚ å·²åŠ è½½ FAISS ç´¢å¼•: {self.index.ntotal} æ¡è®°å½•")
            else:
                # åˆ›å»ºæ–°ç´¢å¼•ï¼ˆä½¿ç”¨ L2 è·ç¦»ï¼‰
                import faiss
                self.index = faiss.IndexFlatL2(self.vector_dim)
                self.logger.info("ğŸ“‚ åˆ›å»ºæ–° FAISS ç´¢å¼•")
            
            # åŠ è½½å…ƒæ•°æ®
            if self.metadata_file.exists():
                with open(self.metadata_file, 'rb') as f:
                    data = pickle.load(f)
                    self.id_to_metadata = data.get('id_to_metadata', {})
                    self.next_id = data.get('next_id', 0)
                self.logger.info(f"ğŸ“‚ å·²åŠ è½½å…ƒæ•°æ®: {len(self.id_to_metadata)} æ¡è®°å½•")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ åŠ è½½ FAISS å­˜å‚¨æ—¶å‡ºé”™: {e}ï¼Œå°†åˆ›å»ºæ–°å­˜å‚¨")
            import faiss
            self.index = faiss.IndexFlatL2(self.vector_dim)
            self.id_to_metadata = {}
            self.next_id = 0
    
    def add(self, content: str, vector: np.ndarray, metadata: Optional[Dict] = None, 
            importance: float = 0.5) -> str:
        """æ·»åŠ è®°å¿†åˆ°å­˜å‚¨
        
        Args:
            content: è®°å¿†å†…å®¹
            vector: å‘é‡ï¼ˆshape: (vector_dim,) æˆ– (1, vector_dim)ï¼‰
            metadata: å…ƒæ•°æ®
            importance: é‡è¦æ€§è¯„åˆ†
            
        Returns:
            str: è®°å¿† ID
        """
        try:
            # ç¡®ä¿å‘é‡æ ¼å¼æ­£ç¡®
            vector = self._prepare_vector(vector)
            
            # ç”Ÿæˆå”¯ä¸€ ID
            memory_id = str(self.next_id)
            self.next_id += 1
            
            # æ·»åŠ åˆ° FAISS ç´¢å¼•
            self.index.add(vector)
            
            # ä¿å­˜å…ƒæ•°æ®
            self.id_to_metadata[memory_id] = {
                'content': content,
                'metadata': metadata or {},
                'timestamp': datetime.now().isoformat(),
                'importance': importance
            }
            
            self.logger.debug(f"âœ… [FAISS] å·²æ·»åŠ è®°å¿† ID={memory_id}: {content[:50]}...")
            
            # å®šæœŸæŒä¹…åŒ–
            if len(self.id_to_metadata) % 10 == 0:
                self._save_to_disk()
            
            return memory_id
            
        except Exception as e:
            self.logger.error(f"âŒ [FAISS] æ·»åŠ è®°å¿†å¤±è´¥: {e}", exc_info=True)
            raise
    
    def search(self, query_vector: np.ndarray, top_k: int = 5, 
               min_importance: float = 0.3) -> List[Tuple[str, float, Dict]]:
        """è¯­ä¹‰æ£€ç´¢
        
        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            top_k: è¿”å›æ•°é‡
            min_importance: æœ€å°é‡è¦æ€§é˜ˆå€¼
            
        Returns:
            List[Tuple[content, similarity_score, metadata]]: æ£€ç´¢ç»“æœ
        """
        try:
            if self.count() == 0:
                return []
            
            # ç¡®ä¿å‘é‡æ ¼å¼æ­£ç¡®
            query_vector = self._prepare_vector(query_vector)
            
            # FAISS æ£€ç´¢
            distances, indices = self.index.search(query_vector, min(top_k * 2, self.count()))
            
            # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆL2 è·ç¦» -> ç›¸ä¼¼åº¦ï¼‰
            results = []
            for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
                if idx == -1:  # FAISS è¿”å› -1 è¡¨ç¤ºæ— ç»“æœ
                    continue
                
                memory_id = str(idx)
                if memory_id not in self.id_to_metadata:
                    continue
                
                meta = self.id_to_metadata[memory_id]
                
                # è¿‡æ»¤ä½é‡è¦æ€§
                if meta['importance'] < min_importance:
                    continue
                
                # è½¬æ¢è·ç¦»ä¸ºç›¸ä¼¼åº¦ï¼ˆè·ç¦»è¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜ï¼‰
                similarity = 1.0 / (1.0 + distance)
                
                results.append((
                    meta['content'],
                    float(similarity),
                    meta['metadata']
                ))
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶é™åˆ¶æ•°é‡
            results.sort(key=lambda x: x[1], reverse=True)
            results = results[:top_k]
            
            self.logger.debug(f"ğŸ” [FAISS] æ£€ç´¢åˆ° {len(results)} æ¡è®°å¿†")
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ [FAISS] æ£€ç´¢å¤±è´¥: {e}", exc_info=True)
            return []
    
    def update(self, memory_id: str, content: Optional[str] = None, 
               vector: Optional[np.ndarray] = None, metadata: Optional[Dict] = None,
               importance: Optional[float] = None):
        """æ›´æ–°è®°å¿†
        
        æ³¨æ„ï¼šFAISS ä¸æ”¯æŒåŸåœ°æ›´æ–°å‘é‡ï¼Œéœ€è¦é‡å»ºç´¢å¼•
        """
        if memory_id not in self.id_to_metadata:
            self.logger.warning(f"âš ï¸ [FAISS] è®°å¿† ID={memory_id} ä¸å­˜åœ¨")
            return
        
        # æ›´æ–°å…ƒæ•°æ®
        meta = self.id_to_metadata[memory_id]
        if content is not None:
            meta['content'] = content
        if metadata is not None:
            meta['metadata'].update(metadata)
        if importance is not None:
            meta['importance'] = importance
        
        # å¦‚æœæ›´æ–°å‘é‡ï¼Œéœ€è¦é‡å»ºç´¢å¼•
        if vector is not None:
            self.logger.warning("âš ï¸ [FAISS] æ›´æ–°å‘é‡éœ€è¦é‡å»ºç´¢å¼•ï¼Œæ“ä½œè¾ƒæ…¢")
            self._rebuild_index()
        
        self.logger.debug(f"âœ… [FAISS] å·²æ›´æ–°è®°å¿† ID={memory_id}")
    
    def delete(self, memory_id: str):
        """åˆ é™¤è®°å¿†
        
        æ³¨æ„ï¼šFAISS ä¸æ”¯æŒåˆ é™¤ï¼Œéœ€è¦é‡å»ºç´¢å¼•
        """
        if memory_id in self.id_to_metadata:
            del self.id_to_metadata[memory_id]
            self._rebuild_index()
            self.logger.debug(f"âœ… [FAISS] å·²åˆ é™¤è®°å¿† ID={memory_id}")
    
    def count(self) -> int:
        """è·å–è®°å¿†æ•°é‡"""
        return self.index.ntotal if self.index else 0
    
    def get_all_metadata(self) -> Dict:
        """è·å–æ‰€æœ‰å…ƒæ•°æ®ï¼ˆç”¨äºå¤‡ä»½ï¼‰"""
        return {
            'id_to_metadata': self.id_to_metadata,
            'next_id': self.next_id,
            'user_id': self.user_id,
            'vector_dim': self.vector_dim,
            'count': self.count(),
            'last_updated': datetime.now().isoformat()
        }
    
    def _prepare_vector(self, vector: np.ndarray) -> np.ndarray:
        """å‡†å¤‡å‘é‡æ ¼å¼ï¼ˆFAISS è¦æ±‚ float32, shape: (1, dim)ï¼‰"""
        if not isinstance(vector, np.ndarray):
            vector = np.array(vector, dtype=np.float32)
        
        if vector.dtype != np.float32:
            vector = vector.astype(np.float32)
        
        if vector.ndim == 1:
            vector = vector.reshape(1, -1)
        
        if vector.shape[1] != self.vector_dim:
            raise ValueError(f"å‘é‡ç»´åº¦ä¸åŒ¹é…: æœŸæœ› {self.vector_dim}, å®é™… {vector.shape[1]}")
        
        return vector
    
    def _rebuild_index(self):
        """é‡å»º FAISS ç´¢å¼•ï¼ˆç”¨äºåˆ é™¤/æ›´æ–°æ“ä½œï¼‰"""
        try:
            import faiss
            from core.ai_services.embedding_service import EmbeddingService
            
            # åˆ›å»ºæ–°ç´¢å¼•
            new_index = faiss.IndexFlatL2(self.vector_dim)
            
            # é‡æ–°ç”Ÿæˆæ‰€æœ‰å‘é‡å¹¶æ·»åŠ 
            embedding_service = EmbeddingService()
            
            for memory_id, meta in self.id_to_metadata.items():
                content = meta['content']
                vector = embedding_service.encode_text([content], convert_to_numpy=True)
                if vector is not None:
                    vector = self._prepare_vector(vector)
                    new_index.add(vector)
            
            self.index = new_index
            self.logger.info(f"âœ… [FAISS] ç´¢å¼•é‡å»ºå®Œæˆ: {self.count()} æ¡è®°å½•")
            
            # ç«‹å³æŒä¹…åŒ–
            self._save_to_disk()
            
        except Exception as e:
            self.logger.error(f"âŒ [FAISS] ç´¢å¼•é‡å»ºå¤±è´¥: {e}", exc_info=True)
    
    def _save_to_disk(self):
        """ä¿å­˜åˆ°ç£ç›˜"""
        try:
            import faiss
            
            # ä¿å­˜ FAISS ç´¢å¼•
            faiss.write_index(self.index, str(self.index_file))
            
            # ä¿å­˜å…ƒæ•°æ®
            with open(self.metadata_file, 'wb') as f:
                pickle.dump({
                    'id_to_metadata': self.id_to_metadata,
                    'next_id': self.next_id
                }, f)
            
            # ä¿å­˜ JSON å¤‡ä»½
            self._backup_to_json()
            
            self.logger.debug(f"ğŸ’¾ [FAISS] å·²ä¿å­˜åˆ°ç£ç›˜: {self.count()} æ¡è®°å½•")
            
        except Exception as e:
            self.logger.error(f"âŒ [FAISS] ä¿å­˜å¤±è´¥: {e}", exc_info=True)
    
    def _backup_to_json(self):
        """å¤‡ä»½åˆ° JSONï¼ˆç”¨äºç¾éš¾æ¢å¤ï¼‰"""
        try:
            backup_data = {
                'user_id': self.user_id,
                'vector_dim': self.vector_dim,
                'count': self.count(),
                'memories': []
            }
            
            for memory_id, meta in self.id_to_metadata.items():
                backup_data['memories'].append({
                    'id': memory_id,
                    'content': meta['content'],
                    'metadata': meta['metadata'],
                    'timestamp': meta['timestamp'],
                    'importance': meta['importance']
                })
            
            with open(self.backup_file, 'w', encoding='utf-8') as f:
                json.dump(backup_data, f, ensure_ascii=False, indent=2)
            
            self.logger.debug(f"ğŸ’¾ [JSONå¤‡ä»½] å·²å¤‡ä»½ {len(backup_data['memories'])} æ¡è®°å¿†")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ [JSONå¤‡ä»½] å¤‡ä»½å¤±è´¥: {e}")
    
    def close(self):
        """å…³é—­å­˜å‚¨ï¼ˆä¿å­˜æ•°æ®ï¼‰"""
        self._save_to_disk()
        self.logger.info("âœ… [FAISS] å­˜å‚¨å·²å…³é—­å¹¶ä¿å­˜")

