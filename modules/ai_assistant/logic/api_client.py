"""
API å®¢æˆ·ç«¯æ¨¡å—
é‡æ„ä¸ºä½¿ç”¨ç­–ç•¥æ¨¡å¼ï¼Œæ”¯æŒå¤šç§ LLM ä¾›åº”å•†ï¼ˆAPI / Ollamaï¼‰
"""

import json
import os
import time
import requests
from PyQt6.QtCore import QThread, pyqtSignal
from typing import Dict, Any, Optional
from pathlib import Path


class APIClient(QThread):
    """
    API å®¢æˆ·ç«¯çº¿ç¨‹ï¼ˆé‡æ„ç‰ˆï¼‰
    
    ä½¿ç”¨ç­–ç•¥æ¨¡å¼ï¼Œé€šè¿‡å·¥å‚åŠ¨æ€é€‰æ‹© LLM ä¾›åº”å•†ï¼ˆAPI / Ollamaï¼‰
    æ”¯æŒæµå¼è¾“å‡º
    """
    # ä¿¡å·å®šä¹‰
    chunk_received = pyqtSignal(str)      # æ¥æ”¶åˆ°æ•°æ®å—
    request_finished = pyqtSignal()       # è¯·æ±‚å®Œæˆ
    token_usage = pyqtSignal(dict)        # Tokenä½¿ç”¨é‡ç»Ÿè®¡
    error_occurred = pyqtSignal(str)      # å‘ç”Ÿé”™è¯¯
    
    def __init__(self, messages, model=None, temperature=None, tools=None, config=None):
        """
        åˆå§‹åŒ– API å®¢æˆ·ç«¯
        
        Args:
            messages: æ¶ˆæ¯å†å²åˆ—è¡¨
            model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œç”¨äºè¦†ç›–é…ç½®ï¼Œå‘åå…¼å®¹ï¼‰
            temperature: æ¸©åº¦å‚æ•°ï¼ˆå¯é€‰ï¼Œå‘åå…¼å®¹ï¼‰
            tools: Function Calling å·¥å…·åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            config: LLM é…ç½®å­—å…¸ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä»é…ç½®æ–‡ä»¶åŠ è½½ï¼‰
        """
        super().__init__()
        self.messages = messages
        self.model = model  # ä¿ç•™ä»¥å‘åå…¼å®¹
        self.temperature = temperature if temperature is not None else 0.8
        self.tools = tools
        self.is_running = True
        
        # åŠ è½½é…ç½®
        self.config = self._load_config() if config is None else config
        
        # åˆ›å»ºç­–ç•¥å®¢æˆ·ç«¯ï¼ˆå»¶è¿Ÿåˆ° run() ä¸­ï¼Œé¿å…åˆå§‹åŒ–é˜»å¡ï¼‰
        self.strategy_client = None
    
    def _load_config(self) -> Dict[str, Any]:
        """ä»é…ç½®æ–‡ä»¶åŠ è½½ AI åŠ©æ‰‹é…ç½®"""
        try:
            from core.config.config_manager import ConfigManager
            
            # è·å–æ¨¡æ¿æ–‡ä»¶è·¯å¾„
            template_path = Path(__file__).parent.parent / "config_template.json"
            
            # å¯¼å…¥é…ç½®æ¨¡å¼
            from modules.ai_assistant.config_schema import get_ai_assistant_schema
            
            # åˆ›å»º ConfigManager å¹¶ä¼ å…¥æ¨¡æ¿è·¯å¾„å’Œé…ç½®æ¨¡å¼
            config_manager = ConfigManager(
                "ai_assistant", 
                template_path=template_path,
                config_schema=get_ai_assistant_schema()  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ é…ç½®æ¨¡å¼
            )
            config = config_manager.get_module_config()
            print(f"[CONFIG] AI åŠ©æ‰‹é…ç½®åŠ è½½æˆåŠŸï¼Œä¾›åº”å•†: {config.get('llm_provider', 'unknown')}")
            return config
        except Exception as e:
            print(f"[ERROR] åŠ è½½é…ç½®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            # ä¸æä¾› fallbackï¼Œå¼ºåˆ¶ç”¨æˆ·é…ç½®
            raise Exception(
                "AI åŠ©æ‰‹é…ç½®åŠ è½½å¤±è´¥ã€‚\n\n"
                "è¯·åœ¨è®¾ç½®ä¸­é…ç½® API Key æˆ– Ollama æœåŠ¡åœ°å€ã€‚\n\n"
                f"é”™è¯¯è¯¦æƒ…: {str(e)}"
            )
    
    def run(self):
        """æ‰§è¡Œ LLM è¯·æ±‚ï¼ˆä½¿ç”¨ç­–ç•¥æ¨¡å¼ï¼‰"""
        # å…³é”®è°ƒè¯•ï¼šè¿½è¸ªAPIClientå¯åŠ¨
        import traceback
        call_stack = ''.join(traceback.format_stack())
        print(f"\n{'='*80}")
        print(f"[API_CLIENT] !!! APIClient.run() è¢«è°ƒç”¨ï¼")
        print(f"[API_CLIENT] æ¶ˆæ¯æ•°é‡: {len(self.messages)}")
        print(f"[API_CLIENT] å·¥å…·æ•°é‡: {len(self.tools) if self.tools else 0}")
        print(f"[API_CLIENT] è°ƒç”¨å †æ ˆ:\n{call_stack}")
        print(f"{'='*80}\n")
        
        try:
            # åˆ›å»ºç­–ç•¥å®¢æˆ·ç«¯
            from modules.ai_assistant.clients import create_llm_client
            
            self.strategy_client = create_llm_client(self.config)
            provider = self.config.get('llm_provider', 'api')
            
            print(f"[LLM] ä½¿ç”¨ä¾›åº”å•†: {provider}, æ¨¡å‹: {self.strategy_client.get_model_name()}")
            
            # è°ƒç”¨ç­–ç•¥ç”Ÿæˆå“åº”
            response_generator = self.strategy_client.generate_response(
                context_messages=self.messages,
                stream=True,
                temperature=self.temperature,
                tools=self.tools
            )
            
            # å¤„ç†ç”Ÿæˆå™¨è¾“å‡ºï¼Œå‘é€ä¿¡å·åˆ° UI
            for chunk in response_generator:
                if not self.is_running:
                    break
                
                if chunk:
                    # æ”¯æŒæ–°æ ¼å¼ï¼ˆdictï¼‰å’Œæ—§æ ¼å¼ï¼ˆstrï¼‰
                    if isinstance(chunk, dict):
                        chunk_type = chunk.get('type')
                        
                        # å¤„ç†æ–‡æœ¬å†…å®¹
                        if chunk_type == 'content':
                            text = chunk.get('text', '')
                            if text:
                                self.chunk_received.emit(text)
                        
                        # âš¡ å¤„ç† token ä½¿ç”¨é‡ç»Ÿè®¡
                        elif chunk_type == 'token_usage':
                            usage = chunk.get('usage', {})
                            self.token_usage.emit(usage)
                        
                        # å¿½ç•¥ tool_calls ç±»å‹ï¼ˆç”±åè°ƒå™¨å¤„ç†ï¼‰
                    else:
                        # æ—§æ ¼å¼ï¼šçº¯å­—ç¬¦ä¸²
                        self.chunk_received.emit(chunk)
            
            # è¯·æ±‚å®Œæˆ
            if self.is_running:
                self.request_finished.emit()
        
        except Exception as e:
            error_msg = str(e)
            print(f"[ERROR] LLM è¯·æ±‚å¤±è´¥: {error_msg}")
            self.error_occurred.emit(error_msg)
    
    def stop(self):
        """åœæ­¢è¯·æ±‚"""
        self.is_running = False
        self.quit()
    
    @staticmethod
    def send(
        messages: list,
        model: str = "gemini-2.5-flash",
        temperature: float = 0.8,
        tools: list = None,
        stream: bool = True
    ):
        """
        v0.2 æ–°å¢ï¼šå‘é€è¯·æ±‚çš„ä¾¿æ·å·¥å‚æ–¹æ³•
        
        å°è£… OpenAI tools å‚æ•°æ ¼å¼ï¼Œç¡®ä¿å…¼å®¹æ€§
        
        Args:
            messages: å¯¹è¯å†å²
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            tools: å·¥å…·åˆ—è¡¨ï¼ˆChatGPT-styleæ ¼å¼ï¼‰
                  æ ¼å¼ï¼š[{type:'function', function:{name, description, parameters}}]
            stream: æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
            
        Returns:
            APIClient: API å®¢æˆ·ç«¯å®ä¾‹
        """
        return APIClient(
            messages=messages,
            model=model,
            temperature=temperature,
            tools=tools
        )

