# -*- coding: utf-8 -*-

"""
ä¸Šä¸‹æ–‡ç®¡ç†å™¨
åè°ƒèµ„äº§ã€æ–‡æ¡£ã€æ—¥å¿—ã€é…ç½®ç­‰å„ç§æ•°æ®æºï¼Œä¸º AI åŠ©æ‰‹æä¾›æ™ºèƒ½ä¸Šä¸‹æ–‡
"""

import re
from typing import Optional, Dict, Any, List
from pathlib import Path
from core.logger import get_logger
from core.ai_services import EmbeddingService

from modules.ai_assistant.logic.asset_reader import AssetReader
from modules.ai_assistant.logic.document_reader import DocumentReader
from modules.ai_assistant.logic.log_analyzer import LogAnalyzer
from modules.ai_assistant.logic.config_reader import ConfigReader
from modules.ai_assistant.logic.site_reader import SiteReader
from modules.ai_assistant.logic.enhanced_memory_manager import EnhancedMemoryManager, MemoryLevel

# å»¶è¿Ÿè·å– logger
def _get_logger():
    return get_logger(__name__)

logger = None

# v0.1 æ–°å¢ï¼šæ„å›¾è§£æã€è¿è¡Œæ€ä¸Šä¸‹æ–‡ã€æœ¬åœ°/è¿œç¨‹æ£€ç´¢
# å¯é€‰å¯¼å…¥ï¼Œå¦‚æœä¾èµ–æœªå®‰è£…åˆ™è·³è¿‡ï¼ˆä¼˜é›…é™çº§ï¼‰
try:
    from modules.ai_assistant.logic.intent_parser import IntentEngine, IntentType
    from modules.ai_assistant.logic.runtime_context import RuntimeContextManager
    from modules.ai_assistant.logic.local_retriever import LocalDocIndex
    from modules.ai_assistant.logic.remote_retriever import RemoteRetriever
    V01_AVAILABLE = True
except ImportError as e:
    # logger æ­¤æ—¶æ˜¯ Noneï¼Œä¸èƒ½è°ƒç”¨ï¼ˆæ¨¡å—çº§ä»£ç ï¼‰
    print(f"[WARNING] v0.1 åŠŸèƒ½ä¸å¯ç”¨ï¼ˆç¼ºå°‘ä¾èµ–ï¼‰ï¼š{e}", flush=True)
    IntentEngine = None
    IntentType = None
    RuntimeContextManager = None
    LocalDocIndex = None
    RemoteRetriever = None
    V01_AVAILABLE = False


class ContextManager:
    """æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆåŸºäº Mem0 è®¾è®¡çš„å¢å¼ºç‰ˆï¼‰
    
    æ ¸å¿ƒèƒ½åŠ›ï¼š
    - å¤šçº§è®°å¿†ç®¡ç†ï¼ˆç”¨æˆ·/ä¼šè¯/ä¸Šä¸‹æ–‡ï¼‰
    - æ™ºèƒ½ä¸Šä¸‹æ–‡èåˆ
    - å»é‡å’Œä¼˜åŒ–
    - ä»æ—¥å¿—å­¦ä¹ 
    """
    
    def __init__(self, asset_manager_logic=None, config_tool_logic=None, site_recommendations_logic=None, runtime_context=None, user_id: str = "default", debug: bool = False, max_context_tokens: int = 4000):
        """åˆå§‹åŒ–ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        
        Args:
            asset_manager_logic: asset_manager æ¨¡å—çš„é€»è¾‘å±‚å®ä¾‹
            config_tool_logic: config_tool æ¨¡å—çš„é€»è¾‘å±‚å®ä¾‹
            site_recommendations_logic: site_recommendations æ¨¡å—çš„é€»è¾‘å±‚å®ä¾‹
            runtime_context: è¿è¡Œæ€ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆå¯é€‰ï¼Œä¼šè‡ªåŠ¨åˆ›å»ºï¼‰
            user_id: ç”¨æˆ·IDï¼ˆç”¨äºè®°å¿†æŒä¹…åŒ–ï¼‰
            debug: æ˜¯å¦å¼€å¯ debug æ¨¡å¼ï¼ˆè¾“å‡ºå®Œæ•´ä¸Šä¸‹æ–‡å¿«ç…§åˆ°æ—¥å¿—ï¼‰
            max_context_tokens: ä¸Šä¸‹æ–‡æœ€å¤§ token æ•°ï¼ˆé»˜è®¤ 4000ï¼Œçº¦2ä¸‡å­—ç¬¦ï¼‰
        """
        # é¦–å…ˆåˆå§‹åŒ– loggerï¼ˆå…¶ä»–æ–¹æ³•éœ€è¦ä½¿ç”¨ï¼‰
        self.logger = _get_logger()
        
        self.asset_reader = AssetReader(asset_manager_logic)
        self.document_reader = DocumentReader()
        self.log_analyzer = LogAnalyzer()
        self.config_reader = ConfigReader(config_tool_logic)
        self.site_reader = SiteReader(site_recommendations_logic)
        
        # åˆ›å»ºç»Ÿä¸€çš„ EmbeddingServiceï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
        self.embedding_service = EmbeddingService()
        
        # åˆå§‹åŒ– ChromaDB å®¢æˆ·ç«¯ï¼ˆç”¨äºå‘é‡å­˜å‚¨ï¼‰
        # æ³¨æ„ï¼šé¿å…è°ƒç”¨ collection.count()ï¼Œä¼šå¯¼è‡´å´©æºƒ
        self.db_client = self._init_chromadb_client()
        
        # å¢å¼ºå‹è®°å¿†ç®¡ç†å™¨ï¼ˆåŸºäº Mem0 è®¾è®¡ï¼Œæ”¯æŒå‘é‡æ£€ç´¢ï¼‰
        self.memory = EnhancedMemoryManager(
            user_id=user_id,
            embedding_service=self.embedding_service,
            db_client=self.db_client
        )
        
        # v0.1 æ–°å¢ï¼šæ„å›¾å¼•æ“ï¼ˆå»¶è¿Ÿåˆ›å»ºï¼Œé¿å…ä¸åå°é¢„åŠ è½½å†²çªï¼‰
        self._intent_engine = None
        self._intent_engine_type = "bge-small" if V01_AVAILABLE and IntentEngine else None
        
        # v0.1 æ–°å¢ï¼šè¿è¡Œæ€ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        self.runtime_context = runtime_context or (RuntimeContextManager() if V01_AVAILABLE and RuntimeContextManager else None)
        
        # v0.1 æ–°å¢ï¼šæœ¬åœ°æ–‡æ¡£ç´¢å¼•ï¼ˆä½¿ç”¨ç»Ÿä¸€çš„åµŒå…¥æœåŠ¡ï¼‰
        self.local_index = LocalDocIndex(embedding_service=self.embedding_service) if V01_AVAILABLE and LocalDocIndex else None
        
        # v0.1 æ–°å¢ï¼šè¿œç¨‹æ£€ç´¢å™¨ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        self.remote_retriever = RemoteRetriever() if V01_AVAILABLE and RemoteRetriever else None
        
        # ä¸Šä¸‹æ–‡ç¼“å­˜ï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
        self._context_cache = {}
        self._cache_ttl = 60  # ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰
        
        # Token æ§åˆ¶
        self.max_context_tokens = max_context_tokens
        
        # Debug æ¨¡å¼
        self.debug = debug
        
        self.logger.info(f"æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼ˆç”¨æˆ·: {user_id}ï¼Œç»Ÿä¸€åµŒå…¥æœåŠ¡: âœ“ï¼Œå‘é‡æ£€ç´¢: âœ“ï¼‰")
    
    def _init_chromadb_client(self):
        """åˆå§‹åŒ– ChromaDB å®¢æˆ·ç«¯"""
        try:
            import chromadb
            from chromadb.config import Settings
            from core.utils.path_utils import PathUtils
            
            # è·å–æ•°æ®åº“è·¯å¾„
            path_utils = PathUtils()
            db_path = path_utils.get_user_data_dir() / "chroma_db"
            db_path.mkdir(parents=True, exist_ok=True)
            
            # åˆ›å»ºæŒä¹…åŒ–å®¢æˆ·ç«¯
            client = chromadb.PersistentClient(
                path=str(db_path),
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True
                )
            )
            
            self.logger.info(f"ChromaDB å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ˆè·¯å¾„: {db_path}ï¼‰")
            return client
            
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ– ChromaDB å®¢æˆ·ç«¯å¤±è´¥: {e}", exc_info=True)
            return None
    
    @property
    def intent_engine(self):
        """å»¶è¿Ÿåˆ›å»ºæ„å›¾å¼•æ“ï¼ˆé¦–æ¬¡è®¿é—®æ—¶åˆ›å»ºï¼Œç¡®ä¿åå°é¢„åŠ è½½å·²å®Œæˆï¼‰"""
        if self._intent_engine is None and self._intent_engine_type and V01_AVAILABLE and IntentEngine:
            self._intent_engine = IntentEngine(model_type=self._intent_engine_type)
            self.logger.info("æ„å›¾å¼•æ“å»¶è¿Ÿåˆ›å»ºå®Œæˆ")
        return self._intent_engine
    
    def analyze_query(self, query: str) -> Dict[str, Any]:
        """åˆ†æç”¨æˆ·æŸ¥è¯¢ï¼Œåˆ¤æ–­éœ€è¦ä»€ä¹ˆç±»å‹çš„ä¸Šä¸‹æ–‡
        
        v0.1 æ›´æ–°ï¼šä½¿ç”¨ IntentEngine è¿›è¡Œè¯­ä¹‰æ„å›¾è§£æ
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            Dict: åˆ†æç»“æœï¼ŒåŒ…å« needs_assets/docs/logs/configs, intent, confidence
        """
        try:
            # v0.1: å¦‚æœæ„å›¾å¼•æ“å¯ç”¨ï¼Œä½¿ç”¨è¯­ä¹‰è§£æ
            if self.intent_engine:
                intent_result = self.intent_engine.parse(query)
                
                intent = intent_result['intent']
                entities = intent_result['entities']
                confidence = intent_result['confidence']
                
                # æ ¹æ®æ„å›¾ç±»å‹æ˜ å°„åˆ°éœ€è¦çš„ä¸Šä¸‹æ–‡
                result = {
                    'needs_assets': intent in [IntentType.ASSET_QUERY, IntentType.ASSET_DETAIL],
                    'needs_docs': intent == IntentType.DOC_SEARCH,
                    'needs_logs': intent in [IntentType.LOG_ANALYZE, IntentType.LOG_SEARCH],
                    'needs_configs': intent in [IntentType.CONFIG_QUERY, IntentType.CONFIG_COMPARE],
                    'needs_sites': intent == IntentType.SITE_RECOMMENDATION,
                    'keywords': entities,
                    'intent': str(intent),
                    'confidence': confidence
                }
                
                return result
            else:
                # v0.1 ä¸å¯ç”¨ï¼Œä½¿ç”¨è§„åˆ™åŒ¹é…
                return self._fallback_analyze(query)
            
        except Exception as e:
            self.logger.error(f"æ„å›¾è§£æå¤±è´¥: {e}", exc_info=True)
            # é™çº§åˆ°è§„åˆ™åŒ¹é…
            return self._fallback_analyze(query)
    
    def _fallback_analyze(self, query: str) -> Dict[str, Any]:
        """è§„åˆ™åŒ¹é…åˆ†æï¼ˆfallbackï¼‰"""
        query_lower = query.lower()
        
        # ç«™ç‚¹æ¨èå…³é”®è¯
        site_keywords = ['ç½‘ç«™', 'ç«™ç‚¹', 'æ¨è', 'èµ„æºç½‘ç«™', 'è®ºå›', 'å­¦ä¹ ç½‘ç«™', 
                         'site', 'website', 'resource', 'forum', 'å“ªé‡Œä¸‹è½½', 
                         'å“ªé‡Œå­¦', 'fab', 'marketplace', 'å•†åŸ', 'èµ„äº§å•†åº—']
        
        needs_sites = any(keyword in query_lower for keyword in site_keywords)
        
        return {
            'needs_assets': False,
            'needs_docs': False,
            'needs_logs': False,
            'needs_configs': False,
            'needs_sites': needs_sites,
            'keywords': [],
            'intent': 'site_recommendation' if needs_sites else 'chitchat',
            'confidence': 0.5 if needs_sites else 0.0
        }
    
    def build_context(self, query: str, include_system_prompt: bool = False) -> str:
        """æ„å»ºæ™ºèƒ½èåˆçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆåŸºäº Mem0 è®¾è®¡ï¼‰
        
        è‡ªåŠ¨èåˆï¼š
        1. ç³»ç»Ÿæç¤ºè¯ï¼ˆé»˜è®¤ä¸åŒ…å«ï¼Œç”±å¤–éƒ¨å•ç‹¬å‘é€ï¼‰
        2. ç”¨æˆ·ç”»åƒï¼ˆä»è®°å¿†æå–ï¼‰
        3. ç›¸å…³å†å²è®°å¿†ï¼ˆæ™ºèƒ½æ£€ç´¢ï¼‰
        4. è¿è¡Œæ—¶çŠ¶æ€ï¼ˆUE å·¥å…·ç®±çŠ¶æ€ï¼‰
        5. ç‰¹å®šé¢†åŸŸä¸Šä¸‹æ–‡ï¼ˆèµ„äº§/é…ç½®/æ—¥å¿—/æ–‡æ¡£ï¼‰
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            include_system_prompt: æ˜¯å¦åŒ…å«ç³»ç»Ÿæç¤ºè¯ï¼ˆé»˜è®¤Falseï¼Œç”±å¤–éƒ¨ç®¡ç†ï¼‰
            
        Returns:
            str: ä¼˜åŒ–åçš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        """
        context_sections = {}  # ä½¿ç”¨å­—å…¸é¿å…é‡å¤
        
        # è°ƒè¯•ï¼šæ˜¾ç¤ºå½“å‰è®°å¿†ç³»ç»ŸçŠ¶æ€
        if hasattr(self.memory, 'user_memories') and hasattr(self.memory, 'session_memories') and hasattr(self.memory, 'context_buffer'):
            self.logger.info(f"[è®°å¿†ç³»ç»ŸçŠ¶æ€] ç”¨æˆ·çº§:{len(self.memory.user_memories)}, ä¼šè¯çº§:{len(self.memory.session_memories)}, ä¸Šä¸‹æ–‡ç¼“å†²:{len(self.memory.context_buffer)}")
        
        # ===== ç¬¬ä¸€å±‚ï¼šæŸ¥è¯¢æ„å›¾åˆ†æï¼ˆæå‰åˆ†æï¼Œä¼˜åŒ–åŠ è½½ç­–ç•¥ï¼‰=====
        analysis = self.analyze_query(query)
        self.logger.info(f"æŸ¥è¯¢æ„å›¾åˆ†æ: {analysis}")
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºç®€å•é—®å€™/é—²èŠ
        is_chitchat = analysis.get('intent') == 'chitchat' or analysis.get('intent') == str(IntentType.CHITCHAT) if V01_AVAILABLE and IntentType else False
        
        # é—²èŠæ¨¡å¼ï¼šè¿”å›åŸºæœ¬è§’è‰²è¯´æ˜+ç›¸å…³è®°å¿†ï¼ˆä»…åœ¨éœ€è¦æ—¶ï¼‰
        if is_chitchat:
            self.logger.info("æ£€æµ‹åˆ°é—²èŠæ¨¡å¼ï¼Œä½¿ç”¨æç®€ä¸Šä¸‹æ–‡")
            
            # Token ä¼˜åŒ–ï¼šå½»åº•ç²¾ç®€é—²èŠæ¨¡å¼çš„ä¸Šä¸‹æ–‡
            # æ™ºèƒ½æ£€ç´¢ç›¸å…³è®°å¿†ï¼ˆä»…åœ¨æ˜ç¡®è¯¢é—®è®°å¿†æ—¶ï¼‰
            is_asking_memory = any(keyword in query.lower() for keyword in ['è®°å¾—', 'è¿˜è®°å¾—', 'è®°ä¸è®°å¾—', 'å¿˜äº†', 'è¯´è¿‡'])
            
            if is_asking_memory:
                # ç”¨æˆ·æ˜ç¡®è¯¢é—®è®°å¿†ï¼Œæ£€ç´¢å¹¶è¿”å›
                relevant_memories = self.memory.get_relevant_memories(query, limit=2, min_importance=0.1)
                
                if relevant_memories:
                    # Tokenä¼˜åŒ–ï¼šåªè¿”å›è®°å¿†ï¼Œä¸åŠ é¢å¤–æè¿°
                    return "[ç›¸å…³è®°å¿†]\n" + "\n".join(f"- {m}" for m in relevant_memories[:2])
                else:
                    return ""  # æ²¡æ‰¾åˆ°å°±è¿”å›ç©ºï¼Œè®©AIè‡ªç„¶å›ç­”
            else:
                # æ™®é€šé—²èŠï¼Œå®Œå…¨ä¸æ·»åŠ ä¸Šä¸‹æ–‡ï¼ˆTokenä¼˜åŒ–ï¼š0 tokenï¼‰
                # è®©AIä¾èµ–ç³»ç»Ÿæç¤ºè¯å’Œå¯¹è¯å†å²è‡ªç„¶å›ç­”
                self.logger.info("æ™®é€šé—²èŠï¼Œè·³è¿‡æ‰€æœ‰ä¸Šä¸‹æ–‡ï¼ˆTokenä¼˜åŒ–ï¼‰")
                return ""
        
        # ===== ç¬¬äºŒå±‚ï¼šç³»ç»Ÿçº§ä¸Šä¸‹æ–‡ï¼ˆä»…åœ¨æ˜¾å¼è¦æ±‚æ—¶æ·»åŠ ï¼‰=====
        if include_system_prompt:
            context_sections['system_prompt'] = self._build_system_prompt(is_chitchat=False)
        
        # ===== ç¬¬ä¸‰å±‚ï¼šç”¨æˆ·èº«ä»½è®¾å®šï¼ˆå§‹ç»ˆåŒ…å«ï¼Œç¡®ä¿AIè®°ä½è§’è‰²ï¼‰=====
        user_identity = self.memory.get_user_identity()
        if user_identity:
            context_sections['user_identity'] = f"[ä½ çš„è§’è‰²èº«ä»½]\n{user_identity}\nâš ï¸ è¯·å§‹ç»ˆä¿æŒè¿™ä¸ªè§’è‰²èº«ä»½ï¼"
            self.logger.info(f"å·²æ·»åŠ ç”¨æˆ·èº«ä»½è®¾å®š: {user_identity[:50]}...")
        
        # ===== ç¬¬å››å±‚ï¼šç”¨æˆ·ç”»åƒï¼ˆä¹ æƒ¯å’Œåå¥½ï¼Œæ’é™¤èº«ä»½ï¼‰=====
        # Tokenä¼˜åŒ–ï¼šç”¨æˆ·ç”»åƒç¼©çŸ­åˆ°300å­—ç¬¦
        user_profile = self.memory.get_user_profile()
        if user_profile:
            context_sections['user_profile'] = user_profile[:300] if len(user_profile) > 300 else user_profile
            self.logger.info("å·²æ·»åŠ ç”¨æˆ·ç”»åƒ")
        
        # ===== ç¬¬äº”å±‚ï¼šæ™ºèƒ½è®°å¿†æ£€ç´¢ =====
        # Tokenä¼˜åŒ–ï¼šä» limit=5 é™åˆ° limit=3ï¼Œæé«˜é˜ˆå€¼ä» 0.2 åˆ° 0.3
        relevant_memories = self.memory.get_relevant_memories(query, limit=3, min_importance=0.3)
        
        # è°ƒè¯•ï¼šè¾“å‡ºè®°å¿†æ£€ç´¢è¯¦æƒ…
        self.logger.info(f"è®°å¿†æ£€ç´¢ç»“æœ: æ‰¾åˆ° {len(relevant_memories)} æ¡è®°å¿†")
        if relevant_memories:
            for i, mem in enumerate(relevant_memories[:5], 1):
                self.logger.info(f"  è®°å¿† {i}: {mem[:100]}...")
        else:
            self.logger.warning("æœªæ‰¾åˆ°ç›¸å…³è®°å¿†ï¼")
        
        if relevant_memories:
            # Tokenä¼˜åŒ–ï¼šæ¯æ¡è®°å¿†é™åˆ¶150å­—ç¬¦ï¼Œæœ€å¤š3æ¡
            truncated_memories = [m[:150] + "..." if len(m) > 150 else m for m in relevant_memories[:3]]
            context_sections['relevant_memories'] = "[ç›¸å…³è®°å¿†]\n" + "\n".join(f"- {m}" for m in truncated_memories)
            self.logger.info(f"å·²æ·»åŠ  {len(truncated_memories)} æ¡è®°å¿†ï¼ˆå·²ç²¾ç®€ï¼‰")
        
        # ===== ç¬¬å…­å±‚ï¼šæœ€è¿‘å¯¹è¯æ‘˜è¦ï¼ˆå¦‚æœæœ‰å‹ç¼©æ‘˜è¦å°±åŒ…å«ï¼‰=====
        if hasattr(self.memory, 'compressed_summary') and self.memory.compressed_summary:
            context_sections['recent_context'] = self.memory.compressed_summary
            self.logger.info("å·²æ·»åŠ å¯¹è¯å†å²æ‘˜è¦")
        
        # ===== ç¬¬ä¸ƒå±‚ï¼šè¿è¡Œæ—¶çŠ¶æ€ï¼ˆä»…åœ¨éœ€è¦èµ„äº§/é…ç½®/æ—¥å¿—æ—¶æ·»åŠ ï¼‰=====
        # Tokenä¼˜åŒ–ï¼šåªåœ¨éœ€è¦æ—¶æ‰æ·»åŠ è¿è¡Œæ—¶çŠ¶æ€ï¼Œä¸”æ›´æ¿€è¿›æˆªæ–­
        if analysis.get('needs_assets') or analysis.get('needs_configs') or analysis.get('needs_logs'):
            if self.runtime_context:
                runtime_snapshot = self.runtime_context.get_formatted_snapshot()
                if runtime_snapshot:
                    # Tokenä¼˜åŒ–ï¼šä» 800 é™åˆ° 400 å­—ç¬¦
                    context_sections['runtime_status'] = runtime_snapshot[:400] if len(runtime_snapshot) > 400 else runtime_snapshot
                    self.logger.info("å·²æ·»åŠ è¿è¡Œæ€å¿«ç…§ï¼ˆç²¾ç®€ç‰ˆï¼‰")
        
        # ===== ç¬¬å…«å±‚ï¼šæ£€ç´¢è¯æ®ï¼ˆæœ¬åœ°ä¼˜å…ˆï¼Œè¿œç¨‹ fallbackï¼‰=====
        # Tokenä¼˜åŒ–ï¼šä»…åœ¨éœ€è¦æ–‡æ¡£æ—¶æ£€ç´¢ï¼ˆèµ„äº§æŸ¥è¯¢ç”¨é¢†åŸŸä¸Šä¸‹æ–‡ï¼‰
        if analysis.get('needs_docs'):
            retrieval_evidence = self._build_retrieval_evidence(query)
            if retrieval_evidence:
                # Tokenä¼˜åŒ–ï¼šä» 1500 é™åˆ° 800 å­—ç¬¦
                context_sections['retrieval_evidence'] = retrieval_evidence[:800] if len(retrieval_evidence) > 800 else retrieval_evidence
                self.logger.info("å·²æ·»åŠ æ£€ç´¢è¯æ®ï¼ˆç²¾ç®€ç‰ˆï¼‰")
        
        # ===== ç¬¬ä¹å±‚ï¼šé¢†åŸŸç‰¹å®šä¸Šä¸‹æ–‡ =====
        domain_contexts = self._build_domain_contexts(query, analysis)
        if domain_contexts:
            context_sections.update(domain_contexts)
        
        # ===== ç¬¬åå±‚ï¼šæ™ºèƒ½å›é€€ =====
        # Tokenä¼˜åŒ–ï¼šå–æ¶ˆè‡ªåŠ¨ fallbackï¼Œåªåœ¨å¿…è¦æ—¶æ‰æ·»åŠ ç³»ç»Ÿæ¦‚è§ˆ
        # è®©AIä¾èµ–ç³»ç»Ÿæç¤ºè¯ä¸­çš„åŸºç¡€ä¿¡æ¯ï¼Œå‡å°‘å†—ä½™
        self.logger.info("è·³è¿‡ fallbackï¼ˆTokenä¼˜åŒ–ï¼‰")
        
        # ===== ä¸Šä¸‹æ–‡ä¼˜åŒ–å’Œå»é‡ =====
        optimized_context = self._optimize_context(context_sections, query)
        
        # v0.1 æ–°å¢ï¼šDebug æ¨¡å¼è¾“å‡º
        if self.debug:
            self.logger.info("="*60)
            self.logger.info("[DEBUG] å®Œæ•´ä¸Šä¸‹æ–‡å¿«ç…§:")
            self.logger.info(f"æŸ¥è¯¢: {query}")
            self.logger.info(f"æ„å›¾: {analysis}")
            self.logger.info(f"ä¸Šä¸‹æ–‡æ®µæ•°: {len(context_sections)}")
            for key, value in context_sections.items():
                self.logger.info(f"  - {key}: {len(value)} å­—ç¬¦")
            self.logger.info(f"ä¼˜åŒ–åé•¿åº¦: {len(optimized_context)} å­—ç¬¦")
            self.logger.info("="*60)
        
        return optimized_context
    
    def _build_asset_context(self, query: str) -> str:
        """æ„å»ºèµ„äº§ç›¸å…³ä¸Šä¸‹æ–‡
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            str: èµ„äº§ä¸Šä¸‹æ–‡
        """
        try:
            query_lower = query.lower()
            
            # æ£€æŸ¥æ˜¯å¦åœ¨è¯¢é—®è¯¦ç»†ä¿¡æ¯/è·¯å¾„
            detail_keywords = ['è·¯å¾„', 'åœ¨å“ª', 'è¯¦ç»†', 'è¯¦æƒ…', 'å…·ä½“', 'åŒ…å«', 'æ–‡ä»¶', 
                              'path', 'detail', 'where', 'location', 'é‡Œé¢æœ‰', 'éƒ½æœ‰ä»€ä¹ˆ']
            is_detail_query = any(keyword in query_lower for keyword in detail_keywords)
            
            # æå–å¯èƒ½çš„èµ„äº§åç§°æˆ–å…³é”®è¯
            keywords = self._extract_keywords(query)
            
            context_parts = []
            
            # å¦‚æœæœ‰æ˜ç¡®çš„æœç´¢å…³é”®è¯
            if keywords:
                for keyword in keywords[:2]:  # æœ€å¤šæœç´¢ 2 ä¸ªå…³é”®è¯
                    # å¦‚æœæ˜¯è¯¦ç»†ä¿¡æ¯æŸ¥è¯¢ï¼Œç›´æ¥è·å–èµ„äº§è¯¦æƒ…
                    if is_detail_query:
                        detail_result = self.asset_reader.get_asset_details(keyword)
                        if "[ERROR]" not in detail_result and "æœªæ‰¾åˆ°" not in detail_result:
                            context_parts.append(detail_result)
                        else:
                            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•æœç´¢
                            search_result = self.asset_reader.search_assets(keyword)
                            if "æ‰¾åˆ°" in search_result:
                                context_parts.append(search_result)
                                context_parts.append("\næç¤ºï¼šå¦‚éœ€æŸ¥çœ‹è¯¦ç»†è·¯å¾„å’Œæ–‡ä»¶åˆ—è¡¨ï¼Œè¯·å‘Šè¯‰æˆ‘å…·ä½“çš„èµ„äº§åç§°ã€‚")
                    else:
                        # å¸¸è§„æœç´¢
                        search_result = self.asset_reader.search_assets(keyword)
                        if "æ‰¾åˆ°" in search_result:
                            context_parts.append(search_result)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šèµ„äº§ï¼Œæä¾›èµ„äº§æ¦‚è§ˆ
            if not context_parts or "æœªæ‰¾åˆ°" in "\n".join(context_parts):
                overview = self.asset_reader.get_all_assets_summary()
                context_parts.insert(0, overview)
            
            return "\n\n".join(context_parts)
        
        except Exception as e:
            self.logger.error(f"æ„å»ºèµ„äº§ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return ""
    
    def _build_document_context(self, query: str) -> str:
        """æ„å»ºæ–‡æ¡£ç›¸å…³ä¸Šä¸‹æ–‡
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            str: æ–‡æ¡£ä¸Šä¸‹æ–‡
        """
        try:
            context_parts = []
            
            # å¦‚æœè¯¢é—®å¦‚ä½•ä½¿ç”¨ï¼Œæä¾› README
            if any(keyword in query.lower() for keyword in ['å¦‚ä½•', 'how', 'æ€ä¹ˆ', 'ä½¿ç”¨']):
                readme = self.document_reader.get_readme_summary()
                context_parts.append(readme)
            else:
                # æœç´¢ç›¸å…³æ–‡æ¡£
                keywords = self._extract_keywords(query)
                for keyword in keywords[:1]:  # åªæœç´¢ä¸»å…³é”®è¯
                    search_result = self.document_reader.search_in_documents(keyword)
                    if "æ‰¾åˆ°" in search_result:
                        context_parts.append(search_result)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œæä¾›æ–‡æ¡£åˆ—è¡¨
            if not context_parts:
                doc_list = self.document_reader.get_available_documents()
                context_parts.append(doc_list)
            
            return "\n\n".join(context_parts)
        
        except Exception as e:
            self.logger.error(f"æ„å»ºæ–‡æ¡£ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return ""
    
    def _build_log_context(self, query: str) -> str:
        """æ„å»ºæ—¥å¿—ç›¸å…³ä¸Šä¸‹æ–‡
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            str: æ—¥å¿—ä¸Šä¸‹æ–‡
        """
        try:
            context_parts = []
            
            # å¦‚æœæ˜ç¡®æåˆ°é”™è¯¯ï¼Œåˆ†æé”™è¯¯
            if any(keyword in query.lower() for keyword in ['é”™è¯¯', 'error', 'å‡ºé”™', 'å¤±è´¥']):
                error_analysis = self.log_analyzer.analyze_errors()
                context_parts.append(error_analysis)
            else:
                # æä¾›æ—¥å¿—æ‘˜è¦
                log_summary = self.log_analyzer.get_log_summary()
                context_parts.append(log_summary)
            
            # å¦‚æœæœ‰ç‰¹å®šå…³é”®è¯ï¼Œæœç´¢æ—¥å¿—
            keywords = self._extract_keywords(query)
            for keyword in keywords[:1]:
                if keyword not in ['é”™è¯¯', 'error', 'log', 'æ—¥å¿—']:
                    search_result = self.log_analyzer.search_in_logs(keyword)
                    if "æ‰¾åˆ°" in search_result:
                        context_parts.append(search_result)
            
            return "\n\n".join(context_parts)
        
        except Exception as e:
            self.logger.error(f"æ„å»ºæ—¥å¿—ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return ""
    
    def _build_config_context(self, query: str) -> str:
        """æ„å»ºé…ç½®ç›¸å…³ä¸Šä¸‹æ–‡
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            str: é…ç½®ä¸Šä¸‹æ–‡
        """
        try:
            context_parts = []
            query_lower = query.lower()
            
            # æ£€æŸ¥æ˜¯å¦åœ¨è¯¢é—®è¯¦ç»†ä¿¡æ¯/è·¯å¾„
            detail_keywords = ['è·¯å¾„', 'åœ¨å“ª', 'è¯¦ç»†', 'è¯¦æƒ…', 'å…·ä½“', 'åŒ…å«', 'æ–‡ä»¶', 
                              'path', 'detail', 'where', 'location', 'é‡Œé¢æœ‰', 'éƒ½æœ‰ä»€ä¹ˆ', 
                              'ini', 'é…ç½®æ–‡ä»¶']
            is_detail_query = any(keyword in query_lower for keyword in detail_keywords)
            
            # æå–å…³é”®è¯
            keywords = self._extract_keywords(query)
            
            # å¦‚æœæœ‰æ˜ç¡®çš„æœç´¢å…³é”®è¯
            if keywords:
                for keyword in keywords[:2]:  # æœ€å¤šæœç´¢ 2 ä¸ªå…³é”®è¯
                    # å¦‚æœæ˜¯è¯¦ç»†ä¿¡æ¯æŸ¥è¯¢ï¼Œç›´æ¥è·å–é…ç½®è¯¦æƒ…
                    if is_detail_query:
                        detail_result = self.config_reader.get_config_details(keyword)
                        if "[ERROR]" not in detail_result and "æœªæ‰¾åˆ°" not in detail_result:
                            context_parts.append(detail_result)
                        else:
                            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•æœç´¢
                            search_result = self.config_reader.search_configs(keyword)
                            if "æ‰¾åˆ°" in search_result:
                                context_parts.append(search_result)
                                context_parts.append("\næç¤ºï¼šå¦‚éœ€æŸ¥çœ‹è¯¦ç»†è·¯å¾„å’Œé…ç½®æ–‡ä»¶åˆ—è¡¨ï¼Œè¯·å‘Šè¯‰æˆ‘å…·ä½“çš„é…ç½®åç§°ã€‚")
                    else:
                        # å¸¸è§„æœç´¢
                        search_result = self.config_reader.search_configs(keyword)
                        if "æ‰¾åˆ°" in search_result:
                            context_parts.append(search_result)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šé…ç½®ï¼Œæä¾›é…ç½®æ¦‚è§ˆ
            if not context_parts or "æœªæ‰¾åˆ°" in "\n".join(context_parts):
                overview = self.config_reader.get_all_configs_summary()
                context_parts.insert(0, overview)
            
            return "\n\n".join(context_parts)
        
        except Exception as e:
            self.logger.error(f"æ„å»ºé…ç½®ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return ""
    
    def _build_site_context(self, query: str) -> str:
        """æ„å»ºç«™ç‚¹æ¨èç›¸å…³ä¸Šä¸‹æ–‡
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            str: ç«™ç‚¹æ¨èä¸Šä¸‹æ–‡
        """
        try:
            query_lower = query.lower()
            
            # æ£€æµ‹ç”¨æˆ·çš„æ„å›¾
            # 1. è¯¢é—®ç‰¹å®šåˆ†ç±»çš„ç«™ç‚¹
            category_keywords = {
                'èµ„æº': 'èµ„æºç½‘ç«™',
                'è®ºå›': 'è®ºå›',
                'å­¦ä¹ ': 'å­¦ä¹ ',
                'å·¥å…·': 'å·¥å…·',
                'resource': 'èµ„æºç½‘ç«™',
                'forum': 'è®ºå›',
                'learn': 'å­¦ä¹ ',
                'tool': 'å·¥å…·'
            }
            
            for keyword, category in category_keywords.items():
                if keyword in query_lower:
                    result = self.site_reader.get_sites_by_category(category)
                    if result and "æœªæ‰¾åˆ°" not in result:
                        return f"[ç«™ç‚¹æ¨è]\n{result}"
            
            # 2. æœç´¢ç‰¹å®šç«™ç‚¹
            search_keywords = ['ç½‘ç«™', 'ç«™ç‚¹', 'æ¨è', 'å“ªé‡Œ', 'å“ªä¸ª', 'site', 'website', 'where']
            if any(keyword in query_lower for keyword in search_keywords):
                # æå–æœç´¢å…³é”®è¯ï¼ˆæ’é™¤å¸¸è§è¯ï¼‰
                excluded_words = ['æ¨è', 'ç½‘ç«™', 'ç«™ç‚¹', 'å“ªé‡Œ', 'å“ªä¸ª', 'æœ‰', 'æ²¡æœ‰', 'å—', 'çš„', 'åœ¨']
                words = [w for w in query.split() if w not in excluded_words and len(w) > 1]
                
                if words:
                    # ä½¿ç”¨ç¬¬ä¸€ä¸ªå…³é”®è¯æœç´¢
                    search_result = self.site_reader.search_sites(words[0])
                    if search_result and "æœªæ‰¾åˆ°" not in search_result:
                        return f"[ç«™ç‚¹æœç´¢ç»“æœ]\n{search_result}"
            
            # 3. é»˜è®¤è¿”å›å…¨éƒ¨ç«™ç‚¹æ‘˜è¦
            return f"[ç«™ç‚¹æ¨è]\n{self.site_reader.get_all_sites_summary(max_count=50)}"
        
        except Exception as e:
            self.logger.error(f"æ„å»ºç«™ç‚¹ä¸Šä¸‹æ–‡å¤±è´¥: {e}", exc_info=True)
            return ""
    
    def _build_system_prompt(self, is_chitchat: bool = False) -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯
        
        Args:
            is_chitchat: æ˜¯å¦ä¸ºé—²èŠæ¨¡å¼
            
        Returns:
            str: ç³»ç»Ÿæç¤ºè¯
        """
        if is_chitchat:
            # é—²èŠæ¨¡å¼ï¼šç®€æ´ç‰ˆæç¤ºè¯
            return """[ç³»ç»Ÿè§’è‰²]
ä½ æ˜¯è™šå¹»å¼•æ“å·¥å…·ç®±çš„ AI åŠ©æ‰‹ã€‚è¯·è‡ªç„¶ã€å‹å¥½åœ°å›ç­”ç”¨æˆ·çš„é—®å€™å’Œé—²èŠã€‚"""
        else:
            # å·¥ä½œæ¨¡å¼ï¼šå®Œæ•´ç‰ˆæç¤ºè¯
            return """[ç³»ç»Ÿè§’è‰²]
ä½ æ˜¯è™šå¹»å¼•æ“å·¥å…·ç®±çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œä¸“æ³¨äºå¸®åŠ©ç”¨æˆ·ç®¡ç† UE é¡¹ç›®èµ„äº§ã€é…ç½®å’Œæ—¥å¿—ã€‚
ä½ æ‹¥æœ‰è®°å¿†èƒ½åŠ›ï¼Œèƒ½è®°ä½ä¹‹å‰çš„å¯¹è¯å†…å®¹ï¼Œå¹¶ç»“åˆç”¨æˆ·ä¹ æƒ¯æä¾›ä¸ªæ€§åŒ–å»ºè®®ã€‚"""
    
    def _build_domain_contexts(self, query: str, analysis: Dict) -> Dict[str, str]:
        """æ„å»ºé¢†åŸŸç‰¹å®šä¸Šä¸‹æ–‡
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            analysis: æŸ¥è¯¢åˆ†æç»“æœ
            
        Returns:
            Dict[str, str]: é¢†åŸŸä¸Šä¸‹æ–‡å­—å…¸
        """
        contexts = {}
        
        if analysis['needs_assets']:
            asset_context = self._build_asset_context(query)
            if asset_context:
                # æ™ºèƒ½é™åˆ¶ï¼šæ£€æµ‹æ˜¯å¦æ˜¯"åˆ—å‡ºæ‰€æœ‰"çš„æŸ¥è¯¢
                query_lower = query.lower()
                is_list_all_query = any(keyword in query_lower for keyword in [
                    'æ‰€æœ‰èµ„äº§', 'å…¨éƒ¨èµ„äº§', 'åˆ—å‡ºèµ„äº§', 'æœ‰å“ªäº›èµ„äº§', 'èµ„äº§åˆ—è¡¨', 
                    'list all', 'show all', 'all assets'
                ])
                
                if is_list_all_query:
                    # ç”¨æˆ·è¦æ±‚åˆ—å‡ºæ‰€æœ‰èµ„äº§ï¼Œä¸æˆªæ–­ï¼
                    contexts['domain_assets'] = asset_context
                    self.logger.info(f"æ£€æµ‹åˆ°åˆ—è¡¨æŸ¥è¯¢ï¼Œä¸é™åˆ¶èµ„äº§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆ{len(asset_context)} å­—ç¬¦ï¼‰")
                else:
                    # å…¶ä»–æŸ¥è¯¢ï¼Œé€‚åº¦é™åˆ¶
                    contexts['domain_assets'] = asset_context[:2000] if len(asset_context) > 2000 else asset_context
                    self.logger.info(f"å¸¸è§„èµ„äº§æŸ¥è¯¢ï¼Œé™åˆ¶åˆ° 2000 å­—ç¬¦")
        
        if analysis['needs_docs']:
            doc_context = self._build_document_context(query)
            if doc_context:
                # Tokenä¼˜åŒ–ï¼šæ–‡æ¡£ä¸Šä¸‹æ–‡é™åˆ¶åˆ° 500 å­—ç¬¦
                contexts['domain_docs'] = doc_context[:500] if len(doc_context) > 500 else doc_context
        
        if analysis['needs_logs']:
            log_context = self._build_log_context(query)
            if log_context:
                # Tokenä¼˜åŒ–ï¼šæ—¥å¿—ä¸Šä¸‹æ–‡é™åˆ¶åˆ° 600 å­—ç¬¦
                contexts['domain_logs'] = log_context[:600] if len(log_context) > 600 else log_context
        
        if analysis['needs_configs']:
            config_context = self._build_config_context(query)
            if config_context:
                # Tokenä¼˜åŒ–ï¼šé…ç½®ä¸Šä¸‹æ–‡é™åˆ¶åˆ° 600 å­—ç¬¦
                contexts['domain_configs'] = config_context[:600] if len(config_context) > 600 else config_context
        
        if analysis.get('needs_sites', False):
            site_context = self._build_site_context(query)
            if site_context:
                # Tokenä¼˜åŒ–ï¼šç«™ç‚¹ä¸Šä¸‹æ–‡é™åˆ¶åˆ° 400 å­—ç¬¦
                contexts['domain_sites'] = site_context[:400] if len(site_context) > 400 else site_context
        
        return contexts
    
    def _build_fallback_context(self, query: str) -> str:
        """æ„å»ºæ™ºèƒ½å›é€€ä¸Šä¸‹æ–‡
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            str: å›é€€ä¸Šä¸‹æ–‡
        """
        fallback_parts = []
        
        # æ·»åŠ ç³»ç»Ÿæ¦‚è§ˆ
        fallback_parts.append(self._build_system_overview())
        
        # å¦‚æœæåˆ°é—®é¢˜ï¼Œè‡ªåŠ¨åŠ è½½é”™è¯¯æ—¥å¿—
        problem_keywords = ['é—®é¢˜', 'é”™è¯¯', 'ä¸å·¥ä½œ', 'æ²¡ååº”', 'å¤±è´¥', 'å´©æºƒ', 
                           'problem', 'error', 'not working', 'fail', 'crash']
        if any(keyword in query.lower() for keyword in problem_keywords):
            log_context = self.log_analyzer.analyze_errors()
            if log_context:
                fallback_parts.append("[è‡ªåŠ¨æ£€æµ‹] æœ€è¿‘çš„é”™è¯¯æ—¥å¿—:\n" + log_context)
        
        return "\n\n".join(fallback_parts) if fallback_parts else ""
    
    def _optimize_context(self, context_sections: Dict[str, str], query: str) -> str:
        """ä¼˜åŒ–å’Œå»é‡ä¸Šä¸‹æ–‡ï¼ˆé¿å…å†—ä½™ï¼‰
        
        Args:
            context_sections: ä¸Šä¸‹æ–‡å„éƒ¨åˆ†çš„å­—å…¸
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            str: ä¼˜åŒ–åçš„ä¸Šä¸‹æ–‡
        """
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        priority_order = [
            'system_prompt',
            'user_identity',      # ç”¨æˆ·èº«ä»½è®¾å®šï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼Œç¡®ä¿AIå§‹ç»ˆè®°ä½è§’è‰²ï¼‰
            'user_profile',
            'relevant_memories',
            'recent_context',
            'runtime_status',
            'domain_assets',
            'domain_configs',
            'domain_logs',
            'domain_docs',
            'domain_sites',       # ç«™ç‚¹æ¨è
            'retrieval_evidence',
            'fallback'
        ]
        
        # æ„å»ºæœ€ç»ˆä¸Šä¸‹æ–‡
        final_parts = []
        total_tokens = 0
        max_tokens = self.max_context_tokens  # ä½¿ç”¨é…ç½®çš„æœ€å¤§ token æ•°
        
        for key in priority_order:
            if key in context_sections and context_sections[key]:
                content = context_sections[key]
                content_tokens = self._estimate_tokens(content)
                
                # æ£€æŸ¥æ˜¯å¦ä¼šè¶…å‡º token é™åˆ¶
                if total_tokens + content_tokens > max_tokens:
                    # æˆªæ–­å†…å®¹ï¼ˆç²—ç•¥ä¼°ç®—ï¼ŒæŒ‰å­—ç¬¦æ¯”ä¾‹æˆªå–ï¼‰
                    remaining_tokens = max_tokens - total_tokens
                    if remaining_tokens > 50:  # è‡³å°‘ä¿ç•™50 tokens
                        ratio = remaining_tokens / content_tokens
                        truncate_length = int(len(content) * ratio)
                        content = content[:truncate_length] + "\n...(å†…å®¹è¢«æˆªæ–­)"
                        final_parts.append(content)
                        total_tokens += remaining_tokens
                    break
                
                final_parts.append(content)
                total_tokens += content_tokens
        
        # æ ¼å¼åŒ–è¾“å‡ºï¼ˆTokenä¼˜åŒ–ï¼šç®€åŒ–header/footerï¼‰
        if final_parts:
            # Tokenä¼˜åŒ–ï¼šå®Œå…¨ç§»é™¤ headerï¼Œç›´æ¥è¿”å›å†…å®¹
            # è®°å½•tokenç»Ÿè®¡åˆ°æ—¥å¿—ï¼Œä¸æ·»åŠ åˆ°è¾“å‡º
            self.logger.info(f"ä¸Šä¸‹æ–‡ Token ç»Ÿè®¡: {total_tokens}/{max_tokens}")
            
            return "\n\n".join(final_parts)
        
        return ""
    
    def _estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—æ–‡æœ¬çš„ token æ•°ï¼ˆç²—ç•¥ä¼°ç®—ï¼‰
        
        ä½¿ç”¨è§„åˆ™ï¼šä¸­æ–‡çº¦ 1.5å­—ç¬¦/tokenï¼Œè‹±æ–‡çº¦ 4å­—ç¬¦/token
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            
        Returns:
            int: ä¼°ç®—çš„ token æ•°
        """
        if not text:
            return 0
        
        # ç»Ÿè®¡ä¸­æ–‡å’Œè‹±æ–‡å­—ç¬¦
        chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
        other_chars = len(text) - chinese_chars
        
        # ä¼°ç®— tokens
        chinese_tokens = chinese_chars / 1.5  # ä¸­æ–‡çº¦1.5å­—ç¬¦/token
        other_tokens = other_chars / 4  # è‹±æ–‡çº¦4å­—ç¬¦/token
        
        return int(chinese_tokens + other_tokens)
    
    def sync_from_log(self, auto_learn: bool = True):
        """ä»æ—¥å¿—è‡ªåŠ¨å­¦ä¹ ä¸Šä¸‹æ–‡ï¼ˆå®ç°"å­¦ä¹ "èƒ½åŠ›ï¼‰
        
        åˆ†ææœ€è¿‘çš„æ—¥å¿—ï¼Œæå–æœ‰ä»·å€¼çš„ä¿¡æ¯ä¿å­˜åˆ°è®°å¿†ä¸­ã€‚
        
        Args:
            auto_learn: æ˜¯å¦è‡ªåŠ¨å­¦ä¹ ï¼ˆè¯„ä¼°é‡è¦æ€§ï¼‰
        """
        self.logger.info("å¼€å§‹ä»æ—¥å¿—å­¦ä¹ ...")
        
        try:
            # 1. è·å–æœ€è¿‘çš„é”™è¯¯æ—¥å¿—
            errors = self.log_analyzer.analyze_errors()
            
            if errors and "é”™è¯¯" in errors:
                # æå–é”™è¯¯æ¨¡å¼
                error_lines = [line for line in errors.split('\n') if 'ERROR' in line or 'é”™è¯¯' in line]
                
                for error_line in error_lines[:5]:  # æœ€å¤šå­¦ä¹ 5æ¡é”™è¯¯
                    # ä¿å­˜åˆ°ç”¨æˆ·çº§è®°å¿†ï¼ˆæŒä¹…åŒ–ï¼‰
                    self.memory.add_memory(
                        content=f"ç³»ç»Ÿé”™è¯¯: {error_line}",
                        level=MemoryLevel.USER,
                        metadata={'type': 'error_log', 'source': 'auto_learn'},
                        auto_evaluate=auto_learn
                    )
                
                self.logger.info(f"ä»æ—¥å¿—ä¸­å­¦ä¹ äº† {len(error_lines[:5])} æ¡é”™è¯¯ä¿¡æ¯")
            
            # 2. è·å–è­¦å‘Šä¿¡æ¯
            warnings = self.log_analyzer.get_log_summary()
            
            if warnings and "WARNING" in warnings:
                # ä¿å­˜è­¦å‘Šæ¨¡å¼åˆ°ä¼šè¯çº§è®°å¿†
                self.memory.add_memory(
                    content=f"ç³»ç»Ÿè­¦å‘Šæ¨¡å¼å·²æ£€æµ‹",
                    level=MemoryLevel.SESSION,
                    metadata={'type': 'warning_pattern', 'source': 'auto_learn'}
                )
            
            self.logger.info("æ—¥å¿—å­¦ä¹ å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"ä»æ—¥å¿—å­¦ä¹ å¤±è´¥: {e}")
    
    def _build_runtime_status(self) -> str:
        """æ„å»ºè¿è¡Œæ—¶çŠ¶æ€ä¿¡æ¯ï¼ˆè½»é‡çº§æ‘˜è¦ï¼Œä¸åŒ…å«è¯¦ç»†åˆ—è¡¨ï¼‰
        
        Returns:
            str: è¿è¡Œæ—¶çŠ¶æ€
        """
        try:
            status_parts = ["[è¿è¡Œæ—¶çŠ¶æ€]"]
            
            # æ£€æŸ¥æœ€è¿‘çš„æ—¥å¿—
            recent_log = self.log_analyzer.get_log_summary()
            if recent_log and "[LOG]" in recent_log:
                status_parts.append(f"**æœ€è¿‘æ—¥å¿—**: å¯ç”¨")
            
            # æ£€æŸ¥èµ„äº§åº“çŠ¶æ€ï¼ˆä»…ç»Ÿè®¡æ•°é‡ï¼Œä¸è·å–è¯¦ç»†åˆ—è¡¨ï¼‰
            try:
                if self.asset_reader.asset_manager_logic:
                    assets = self.asset_reader.asset_manager_logic.get_all_assets()
                    if assets:
                        status_parts.append(f"**èµ„äº§åº“**: {len(assets)} ä¸ªèµ„äº§")
            except Exception as e:
                self.logger.debug(f"è·å–èµ„äº§ç»Ÿè®¡å¤±è´¥: {e}")
            
            # æ£€æŸ¥é…ç½®çŠ¶æ€ï¼ˆä»…ç»Ÿè®¡æ•°é‡ï¼‰
            try:
                if self.config_reader.config_tool_logic:
                    configs = self.config_reader.config_tool_logic.get_all_templates()
                    if configs:
                        status_parts.append(f"**é…ç½®æ¨¡æ¿**: {len(configs)} ä¸ªé…ç½®")
            except Exception as e:
                self.logger.debug(f"è·å–é…ç½®ç»Ÿè®¡å¤±è´¥: {e}")
            
            if len(status_parts) > 1:
                return "\n".join(status_parts)
            
            return ""
        
        except Exception as e:
            self.logger.error(f"æ„å»ºè¿è¡Œæ—¶çŠ¶æ€å¤±è´¥: {e}")
            return ""
    
    def _build_system_overview(self) -> str:
        """æ„å»ºç³»ç»Ÿæ¦‚è§ˆ
        
        Returns:
            str: ç³»ç»Ÿæ¦‚è§ˆ
        """
        try:
            # Tokenä¼˜åŒ–ï¼šå¤§å¹…ç²¾ç®€ç³»ç»Ÿæ¦‚è§ˆ
            overview_parts = [
                "[ç³»ç»Ÿèƒ½åŠ›]",
                "èµ„äº§ç®¡ç†ã€é…ç½®æŸ¥è¯¢ã€æ–‡æ¡£æŸ¥é˜…ã€æ—¥å¿—åˆ†æ"
            ]
            
            return "\n".join(overview_parts)
        
        except Exception as e:
            self.logger.error(f"æ„å»ºç³»ç»Ÿæ¦‚è§ˆå¤±è´¥: {e}")
            return ""
    
    def _extract_keywords(self, query: str) -> List[str]:
        """ä»æŸ¥è¯¢ä¸­æå–å…³é”®è¯
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            List[str]: å…³é”®è¯åˆ—è¡¨
        """
        # ç§»é™¤å¸¸è§çš„åœç”¨è¯
        stop_words = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'å—', 'å‘¢', 'å•Š', 'å§', 
                     'the', 'a', 'an', 'is', 'are', 'what', 'how', 'where'}
        
        # åˆ†è¯ï¼ˆç®€å•å®ç°ï¼‰
        words = re.findall(r'\w+', query.lower())
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        keywords = [word for word in words if word not in stop_words and len(word) > 1]
        
        return keywords
    
    def _build_retrieval_evidence(self, query: str) -> str:
        """
        æ„å»ºæ£€ç´¢è¯æ®ï¼ˆæœ¬åœ°ä¼˜å…ˆï¼Œè¿œç¨‹ fallbackï¼‰
        
        v0.1 æ–°å¢ï¼šé›†æˆæœ¬åœ°æ–‡æ¡£æ£€ç´¢å’Œè¿œç¨‹ä»£ç æœç´¢
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            str: æ£€ç´¢è¯æ®æ–‡æœ¬ï¼Œé™„å¸¦æ¥æºæ ‡ç­¾
        """
        try:
            evidence_parts = []
            
            # 1. ä¼˜å…ˆæœ¬åœ°æ–‡æ¡£æ£€ç´¢
            try:
                if self.local_index:
                    self.logger.info("ğŸ“– [ä¸Šä¸‹æ–‡æ„å»º] è°ƒç”¨æœ¬åœ°æ–‡æ¡£å‘é‡æ£€ç´¢...")
                    # Tokenä¼˜åŒ–ï¼šä» top_k=3 é™åˆ° top_k=2
                    local_results = self.local_index.search(query, top_k=2)
                else:
                    self.logger.warning("âš ï¸ [ä¸Šä¸‹æ–‡æ„å»º] æœ¬åœ°æ–‡æ¡£ç´¢å¼•æœªå¯ç”¨")
                    local_results = []
                
                if local_results:
                    evidence_parts.append("[æ–‡æ¡£]")
                    for i, result in enumerate(local_results, 1):
                        # Tokenä¼˜åŒ–ï¼šä» 150 é™åˆ° 120 å­—ç¬¦
                        content = result['content'][:120]
                        metadata = result.get('metadata', {})
                        source = metadata.get('source', 'unknown')
                        
                        # Tokenä¼˜åŒ–ï¼šæ›´ç®€æ´çš„æ ¼å¼
                        evidence_parts.append(f"{i}. {content}...")
                    
                    self.logger.info(f"âœ… [ä¸Šä¸‹æ–‡æ„å»º] æœ¬åœ°æ–‡æ¡£å‘é‡æ£€ç´¢è¿”å› {len(local_results)} æ¡ç»“æœ")
            
            except Exception as e:
                self.logger.warning(f"æœ¬åœ°æ£€ç´¢å¤±è´¥: {e}")
            
            # 2. å¦‚æœæœ¬åœ°æ— ç»“æœï¼Œå°è¯•è¿œç¨‹æ£€ç´¢ï¼ˆGitHubï¼‰
            if not evidence_parts:
                try:
                    # åªåœ¨ç‰¹å®šæ„å›¾ä¸‹å¯ç”¨è¿œç¨‹æœç´¢ï¼ˆé¿å…è¿‡åº¦è°ƒç”¨ APIï¼‰
                    analysis = self.analyze_query(query)
                    if self.remote_retriever and analysis.get('intent') in ['doc.search', 'asset.query']:
                        remote_results = self.remote_retriever.search(
                            query, 
                            source="github",
                            repo="EpicGames/UnrealEngine",  # å¯é…ç½®
                            top_k=2
                        )
                    else:
                        remote_results = []
                    
                    if remote_results:
                        evidence_parts.append("[è¿œç¨‹ä»£ç ]")
                        for i, result in enumerate(remote_results, 1):
                            # Tokenä¼˜åŒ–ï¼šä» 200 é™åˆ° 100 å­—ç¬¦ï¼Œç§»é™¤å†—ä½™ä¿¡æ¯
                            snippet = result.get('content_snippet', '')[:100]
                            path = result.get('path', '')
                            
                            evidence_parts.append(f"{i}. {snippet}... ({path})")
                        
                        self.logger.info(f"è¿œç¨‹æ£€ç´¢åˆ° {len(remote_results)} æ¡ç»“æœ")
                
                except Exception as e:
                    self.logger.warning(f"è¿œç¨‹æ£€ç´¢å¤±è´¥ï¼ˆå¯èƒ½æœªé…ç½® tokenï¼‰: {e}")
            
            return "\n".join(evidence_parts) if evidence_parts else ""
        
        except Exception as e:
            self.logger.error(f"æ„å»ºæ£€ç´¢è¯æ®å¤±è´¥: {e}", exc_info=True)
            return ""
    
    def execute_command(self, command: str) -> Optional[str]:
        """æ‰§è¡Œç‰¹å®šå‘½ä»¤
        
        Args:
            command: å‘½ä»¤å­—ç¬¦ä¸²
            
        Returns:
            Optional[str]: å‘½ä»¤æ‰§è¡Œç»“æœ
        """
        command_lower = command.lower()
        
        # èµ„äº§ç›¸å…³å‘½ä»¤
        if 'èµ„äº§' in command_lower or 'asset' in command_lower:
            if 'åˆ—è¡¨' in command_lower or 'æ‰€æœ‰' in command_lower or 'æœ‰å“ªäº›' in command_lower:
                return self.asset_reader.get_all_assets_summary()
            elif 'åˆ†ç±»' in command_lower:
                return self.asset_reader.get_categories_list()
        
        # æ–‡æ¡£ç›¸å…³å‘½ä»¤
        if 'æ–‡æ¡£' in command_lower or 'document' in command_lower:
            if 'åˆ—è¡¨' in command_lower or 'æœ‰å“ªäº›' in command_lower:
                return self.document_reader.get_available_documents()
        
        # æ—¥å¿—ç›¸å…³å‘½ä»¤
        if 'æ—¥å¿—' in command_lower or 'log' in command_lower:
            if 'é”™è¯¯' in command_lower or 'error' in command_lower:
                return self.log_analyzer.analyze_errors()
            elif 'ç»Ÿè®¡' in command_lower or 'æ‘˜è¦' in command_lower:
                return self.log_analyzer.get_log_summary()
        
        return None




