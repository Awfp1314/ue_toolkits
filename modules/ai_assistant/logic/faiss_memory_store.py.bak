"""
FAISS å‘é‡å­˜å‚¨ç®¡ç†å™¨

åŠŸèƒ½ï¼š
- ä½¿ç”¨ FAISS ä½œä¸ºä¸»å­˜å‚¨å¼•æ“
- é«˜æ•ˆçš„å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢
- æ”¯æŒå…ƒæ•°æ®å­˜å‚¨å’Œè¿‡æ»¤
- è‡ªåŠ¨æŒä¹…åŒ–åˆ°ç£ç›˜
- JSON å¤‡ä»½æœºåˆ¶

ä½œè€…ï¼šAI Assistant
æ—¥æœŸï¼š2025-11-06
"""

import json
import pickle
import numpy as np
from pathlib import Path
from typing import List, Dict, Optional, Any, Tuple
from datetime import datetime
import logging


def _get_logger():
    """è·å–æ—¥å¿—è®°å½•å™¨"""
    return logging.getLogger("ue_toolkit.modules.ai_assistant.logic.faiss_memory_store")


class FaissMemoryStore:
    """FAISS å‘é‡å­˜å‚¨ç®¡ç†å™¨
    
    æ¶æ„è®¾è®¡ï¼š
    1. FAISS ç´¢å¼•ï¼šå­˜å‚¨å‘é‡ï¼Œæ”¯æŒé«˜æ•ˆæ£€ç´¢
    2. å…ƒæ•°æ®æ˜ å°„ï¼šID -> {content, metadata, timestamp, importance}
    3. ç£ç›˜æŒä¹…åŒ–ï¼šå®šæœŸä¿å­˜åˆ°ç£ç›˜
    4. JSON å¤‡ä»½ï¼šç¾éš¾æ¢å¤
    """
    
    def __init__(self, storage_dir: Path, vector_dim: int = 512, user_id: str = "default",
                 batch_delete_threshold: int = 50):
        """åˆå§‹åŒ– FAISS å­˜å‚¨

        Args:
            storage_dir: å­˜å‚¨ç›®å½•
            vector_dim: å‘é‡ç»´åº¦ï¼ˆé»˜è®¤ 512ï¼ŒåŒ¹é… bge-small-zh-v1.5ï¼‰
            user_id: ç”¨æˆ· ID
            batch_delete_threshold: æ‰¹é‡åˆ é™¤é˜ˆå€¼ï¼ˆé»˜è®¤ 50ï¼‰
        """
        self.logger = _get_logger()
        self.storage_dir = Path(storage_dir)
        self.storage_dir.mkdir(parents=True, exist_ok=True)

        self.user_id = user_id
        self.vector_dim = vector_dim
        self.batch_delete_threshold = batch_delete_threshold

        # æ–‡ä»¶è·¯å¾„
        self.index_file = self.storage_dir / f"{user_id}_faiss.index"
        self.metadata_file = self.storage_dir / f"{user_id}_metadata.pkl"
        self.backup_file = self.storage_dir / f"{user_id}_backup.json"

        # åˆå§‹åŒ– FAISS ç´¢å¼•
        self.index = None
        self.id_to_metadata = {}  # {memory_id: {content, metadata, timestamp, importance, deleted}}
        self.next_id = 0

        # âš¡ Requirement 7.1, 7.2: å»¶è¿Ÿåˆ é™¤ä¼˜åŒ–
        self._pending_deletes = 0  # å¾…åˆ é™¤è®¡æ•°å™¨

        # åŠ è½½ç°æœ‰æ•°æ®
        self._load_from_disk()

        self.logger.info(f"âœ… FAISS å­˜å‚¨åˆå§‹åŒ–å®Œæˆï¼ˆç”¨æˆ·: {user_id}, å‘é‡ç»´åº¦: {vector_dim}, å·²æœ‰è®°å¿†: {self.count()}, æ‰¹é‡åˆ é™¤é˜ˆå€¼: {batch_delete_threshold}ï¼‰")
    
    def _load_from_disk(self):
        """ä»ç£ç›˜åŠ è½½ç´¢å¼•å’Œå…ƒæ•°æ®"""
        try:
            # å°è¯•åŠ è½½ FAISS ç´¢å¼•
            if self.index_file.exists():
                import faiss
                self.index = faiss.read_index(str(self.index_file))
                self.logger.info(f"ğŸ“‚ å·²åŠ è½½ FAISS ç´¢å¼•: {self.index.ntotal} æ¡è®°å½•")
            else:
                # åˆ›å»ºæ–°ç´¢å¼•ï¼ˆä½¿ç”¨ L2 è·ç¦»ï¼‰
                import faiss
                self.index = faiss.IndexFlatL2(self.vector_dim)
                self.logger.info("ğŸ“‚ åˆ›å»ºæ–° FAISS ç´¢å¼•")
            
            # åŠ è½½å…ƒæ•°æ®
            if self.metadata_file.exists():
                with open(self.metadata_file, 'rb') as f:
                    data = pickle.load(f)
                    self.id_to_metadata = data.get('id_to_metadata', {})
                    self.next_id = data.get('next_id', 0)
                    # âš¡ Requirement 7.5: åŠ è½½å¾…åˆ é™¤è®¡æ•°å™¨
                    self._pending_deletes = data.get('pending_deletes', 0)
                self.logger.info(f"ğŸ“‚ å·²åŠ è½½å…ƒæ•°æ®: {len(self.id_to_metadata)} æ¡è®°å½•ï¼ˆå¾…åˆ é™¤: {self._pending_deletes}ï¼‰")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ åŠ è½½ FAISS å­˜å‚¨æ—¶å‡ºé”™: {e}ï¼Œå°†åˆ›å»ºæ–°å­˜å‚¨")
            import faiss
            self.index = faiss.IndexFlatL2(self.vector_dim)
            self.id_to_metadata = {}
            self.next_id = 0
    
    def add(self, content: str, vector: np.ndarray, metadata: Optional[Dict] = None, 
            importance: float = 0.5) -> str:
        """æ·»åŠ è®°å¿†åˆ°å­˜å‚¨
        
        Args:
            content: è®°å¿†å†…å®¹
            vector: å‘é‡ï¼ˆshape: (vector_dim,) æˆ– (1, vector_dim)ï¼‰
            metadata: å…ƒæ•°æ®
            importance: é‡è¦æ€§è¯„åˆ†
            
        Returns:
            str: è®°å¿† ID
        """
        try:
            # ç¡®ä¿å‘é‡æ ¼å¼æ­£ç¡®
            vector = self._prepare_vector(vector)
            
            # ç”Ÿæˆå”¯ä¸€ ID
            memory_id = str(self.next_id)
            self.next_id += 1
            
            # æ·»åŠ åˆ° FAISS ç´¢å¼•
            self.index.add(vector)
            
            # ä¿å­˜å…ƒæ•°æ®
            self.id_to_metadata[memory_id] = {
                'content': content,
                'metadata': metadata or {},
                'timestamp': datetime.now().isoformat(),
                'importance': importance,
                'deleted': False  # âš¡ Requirement 7.1: æ·»åŠ åˆ é™¤æ ‡è®°
            }
            
            self.logger.debug(f"âœ… [FAISS] å·²æ·»åŠ è®°å¿† ID={memory_id}: {content[:50]}...")
            
            # å®šæœŸæŒä¹…åŒ–
            if len(self.id_to_metadata) % 10 == 0:
                self._save_to_disk()
            
            return memory_id
            
        except Exception as e:
            self.logger.error(f"âŒ [FAISS] æ·»åŠ è®°å¿†å¤±è´¥: {e}", exc_info=True)
            raise
    
    def search(self, query_vector: np.ndarray, top_k: int = 5, 
               min_importance: float = 0.3) -> List[Tuple[str, float, Dict]]:
        """è¯­ä¹‰æ£€ç´¢
        
        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            top_k: è¿”å›æ•°é‡
            min_importance: æœ€å°é‡è¦æ€§é˜ˆå€¼
            
        Returns:
            List[Tuple[content, similarity_score, metadata]]: æ£€ç´¢ç»“æœ
        """
        try:
            if self.count() == 0:
                return []
            
            # ç¡®ä¿å‘é‡æ ¼å¼æ­£ç¡®
            query_vector = self._prepare_vector(query_vector)
            
            # FAISS æ£€ç´¢
            distances, indices = self.index.search(query_vector, min(top_k * 2, self.count()))
            
            # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆL2 è·ç¦» -> ç›¸ä¼¼åº¦ï¼‰
            results = []
            for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
                if idx == -1:  # FAISS è¿”å› -1 è¡¨ç¤ºæ— ç»“æœ
                    continue

                memory_id = str(idx)
                if memory_id not in self.id_to_metadata:
                    continue

                meta = self.id_to_metadata[memory_id]

                # âš¡ Requirement 7.3: è¿‡æ»¤å·²åˆ é™¤çš„è®°å¿†
                if meta.get('deleted', False):
                    continue

                # è¿‡æ»¤ä½é‡è¦æ€§
                if meta['importance'] < min_importance:
                    continue
                
                # è½¬æ¢è·ç¦»ä¸ºç›¸ä¼¼åº¦ï¼ˆè·ç¦»è¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜ï¼‰
                similarity = 1.0 / (1.0 + distance)
                
                results.append((
                    meta['content'],
                    float(similarity),
                    meta['metadata']
                ))
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶é™åˆ¶æ•°é‡
            results.sort(key=lambda x: x[1], reverse=True)
            results = results[:top_k]
            
            self.logger.debug(f"ğŸ” [FAISS] æ£€ç´¢åˆ° {len(results)} æ¡è®°å¿†")
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ [FAISS] æ£€ç´¢å¤±è´¥: {e}", exc_info=True)
            return []
    
    def update(self, memory_id: str, content: Optional[str] = None, 
               vector: Optional[np.ndarray] = None, metadata: Optional[Dict] = None,
               importance: Optional[float] = None):
        """æ›´æ–°è®°å¿†
        
        æ³¨æ„ï¼šFAISS ä¸æ”¯æŒåŸåœ°æ›´æ–°å‘é‡ï¼Œéœ€è¦é‡å»ºç´¢å¼•
        """
        if memory_id not in self.id_to_metadata:
            self.logger.warning(f"âš ï¸ [FAISS] è®°å¿† ID={memory_id} ä¸å­˜åœ¨")
            return
        
        # æ›´æ–°å…ƒæ•°æ®
        meta = self.id_to_metadata[memory_id]
        if content is not None:
            meta['content'] = content
        if metadata is not None:
            meta['metadata'].update(metadata)
        if importance is not None:
            meta['importance'] = importance
        
        # å¦‚æœæ›´æ–°å‘é‡ï¼Œéœ€è¦é‡å»ºç´¢å¼•
        if vector is not None:
            self.logger.warning("âš ï¸ [FAISS] æ›´æ–°å‘é‡éœ€è¦é‡å»ºç´¢å¼•ï¼Œæ“ä½œè¾ƒæ…¢")
            self._rebuild_index()
        
        self.logger.debug(f"âœ… [FAISS] å·²æ›´æ–°è®°å¿† ID={memory_id}")
    
    def delete(self, memory_id: str):
        """åˆ é™¤è®°å¿†

        âš¡ Requirement 7.1, 7.2: ä½¿ç”¨å»¶è¿Ÿåˆ é™¤ä¼˜åŒ–
        - æ ‡è®°ä¸ºåˆ é™¤è€Œä¸æ˜¯ç«‹å³é‡å»ºç´¢å¼•
        - è¾¾åˆ°é˜ˆå€¼æ—¶æ‰¹é‡é‡å»º
        """
        if memory_id not in self.id_to_metadata:
            self.logger.warning(f"âš ï¸ [FAISS] è®°å¿† ID={memory_id} ä¸å­˜åœ¨")
            return

        # æ ‡è®°ä¸ºåˆ é™¤
        if not self.id_to_metadata[memory_id].get('deleted', False):
            self.id_to_metadata[memory_id]['deleted'] = True
            self._pending_deletes += 1
            self.logger.debug(f"âœ… [FAISS] å·²æ ‡è®°åˆ é™¤ ID={memory_id}ï¼ˆå¾…åˆ é™¤: {self._pending_deletes}/{self.batch_delete_threshold}ï¼‰")

            # è¾¾åˆ°é˜ˆå€¼æ—¶æ‰¹é‡é‡å»º
            if self._pending_deletes >= self.batch_delete_threshold:
                self.logger.info(f"ğŸ”„ [FAISS] è¾¾åˆ°æ‰¹é‡åˆ é™¤é˜ˆå€¼ï¼ˆ{self._pending_deletes}ï¼‰ï¼Œå¼€å§‹é‡å»ºç´¢å¼•...")
                self._rebuild_index()
                self._pending_deletes = 0
            else:
                # æœªè¾¾åˆ°é˜ˆå€¼ï¼Œåªä¿å­˜å…ƒæ•°æ®
                self._save_to_disk()
    
    def count(self) -> int:
        """è·å–è®°å¿†æ•°é‡ï¼ˆåŒ…æ‹¬å·²åˆ é™¤ä½†æœªæ¸…ç†çš„ï¼‰"""
        return self.index.ntotal if self.index else 0

    def count_active(self) -> int:
        """è·å–æ´»è·ƒè®°å¿†æ•°é‡ï¼ˆä¸åŒ…æ‹¬å·²åˆ é™¤çš„ï¼‰

        âš¡ Requirement 7.3: æä¾›æ´»è·ƒè®°å¿†è®¡æ•°
        """
        return sum(1 for meta in self.id_to_metadata.values() if not meta.get('deleted', False))

    def get_stats(self) -> Dict[str, Any]:
        """è·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯

        Returns:
            Dict: {
                'total': æ€»è®°å¿†æ•°ï¼ˆåŒ…æ‹¬å·²åˆ é™¤ï¼‰,
                'active': æ´»è·ƒè®°å¿†æ•°,
                'deleted': å·²åˆ é™¤ä½†æœªæ¸…ç†çš„è®°å¿†æ•°,
                'pending_deletes': å¾…åˆ é™¤è®¡æ•°å™¨,
                'threshold': æ‰¹é‡åˆ é™¤é˜ˆå€¼
            }
        """
        total = len(self.id_to_metadata)
        active = self.count_active()
        deleted = total - active

        return {
            'total': total,
            'active': active,
            'deleted': deleted,
            'pending_deletes': self._pending_deletes,
            'threshold': self.batch_delete_threshold,
            'needs_rebuild': self._pending_deletes >= self.batch_delete_threshold
        }
    
    def get_all_metadata(self) -> Dict:
        """è·å–æ‰€æœ‰å…ƒæ•°æ®ï¼ˆç”¨äºå¤‡ä»½ï¼‰"""
        return {
            'id_to_metadata': self.id_to_metadata,
            'next_id': self.next_id,
            'user_id': self.user_id,
            'vector_dim': self.vector_dim,
            'count': self.count(),
            'last_updated': datetime.now().isoformat()
        }
    
    def _prepare_vector(self, vector: np.ndarray) -> np.ndarray:
        """å‡†å¤‡å‘é‡æ ¼å¼ï¼ˆFAISS è¦æ±‚ float32, shape: (1, dim)ï¼‰"""
        if not isinstance(vector, np.ndarray):
            vector = np.array(vector, dtype=np.float32)
        
        if vector.dtype != np.float32:
            vector = vector.astype(np.float32)
        
        if vector.ndim == 1:
            vector = vector.reshape(1, -1)
        
        if vector.shape[1] != self.vector_dim:
            raise ValueError(f"å‘é‡ç»´åº¦ä¸åŒ¹é…: æœŸæœ› {self.vector_dim}, å®é™… {vector.shape[1]}")
        
        return vector
    
    def _rebuild_index(self):
        """é‡å»º FAISS ç´¢å¼•ï¼ˆç”¨äºåˆ é™¤/æ›´æ–°æ“ä½œï¼‰

        âš¡ Requirement 7.4: é‡å»ºæ—¶ç§»é™¤å·²åˆ é™¤çš„è®°å¿†
        """
        try:
            import faiss
            from core.ai_services.embedding_service import EmbeddingService

            # âš¡ ç¬¬ä¸€æ­¥ï¼šç§»é™¤å·²åˆ é™¤çš„è®°å¿†
            deleted_ids = [mid for mid, meta in self.id_to_metadata.items() if meta.get('deleted', False)]
            for memory_id in deleted_ids:
                del self.id_to_metadata[memory_id]

            if deleted_ids:
                self.logger.info(f"ğŸ—‘ï¸ [FAISS] é‡å»ºæ—¶ç§»é™¤ {len(deleted_ids)} æ¡å·²åˆ é™¤è®°å¿†")

            # åˆ›å»ºæ–°ç´¢å¼•
            new_index = faiss.IndexFlatL2(self.vector_dim)

            # é‡æ–°ç”Ÿæˆæ‰€æœ‰å‘é‡å¹¶æ·»åŠ ï¼ˆåªæ·»åŠ æœªåˆ é™¤çš„ï¼‰
            embedding_service = EmbeddingService()

            for memory_id, meta in self.id_to_metadata.items():
                # è·³è¿‡å·²åˆ é™¤çš„ï¼ˆé˜²å¾¡æ€§æ£€æŸ¥ï¼‰
                if meta.get('deleted', False):
                    continue

                content = meta['content']
                vector = embedding_service.encode_text([content], convert_to_numpy=True)
                if vector is not None:
                    vector = self._prepare_vector(vector)
                    new_index.add(vector)

            self.index = new_index
            self.logger.info(f"âœ… [FAISS] ç´¢å¼•é‡å»ºå®Œæˆ: {self.count()} æ¡è®°å½•")

            # ç«‹å³æŒä¹…åŒ–
            self._save_to_disk()

        except Exception as e:
            self.logger.error(f"âŒ [FAISS] ç´¢å¼•é‡å»ºå¤±è´¥: {e}", exc_info=True)
    
    def _save_to_disk(self):
        """ä¿å­˜åˆ°ç£ç›˜

        âš¡ Requirement 7.5: æŒä¹…åŒ–åˆ é™¤æ ‡è®°å’Œå¾…åˆ é™¤è®¡æ•°å™¨
        """
        try:
            import faiss

            # ä¿å­˜ FAISS ç´¢å¼•
            faiss.write_index(self.index, str(self.index_file))

            # ä¿å­˜å…ƒæ•°æ®ï¼ˆåŒ…æ‹¬åˆ é™¤æ ‡è®°å’Œå¾…åˆ é™¤è®¡æ•°å™¨ï¼‰
            with open(self.metadata_file, 'wb') as f:
                pickle.dump({
                    'id_to_metadata': self.id_to_metadata,
                    'next_id': self.next_id,
                    'pending_deletes': self._pending_deletes  # âš¡ æŒä¹…åŒ–å¾…åˆ é™¤è®¡æ•°å™¨
                }, f)
            
            # ä¿å­˜ JSON å¤‡ä»½
            self._backup_to_json()
            
            self.logger.debug(f"ğŸ’¾ [FAISS] å·²ä¿å­˜åˆ°ç£ç›˜: {self.count()} æ¡è®°å½•")
            
        except Exception as e:
            self.logger.error(f"âŒ [FAISS] ä¿å­˜å¤±è´¥: {e}", exc_info=True)
    
    def _backup_to_json(self):
        """å¤‡ä»½åˆ° JSONï¼ˆç”¨äºç¾éš¾æ¢å¤ï¼‰

        âš¡ Requirement 7.5: å¤‡ä»½æ—¶è·³è¿‡å·²åˆ é™¤çš„è®°å¿†
        """
        try:
            backup_data = {
                'user_id': self.user_id,
                'vector_dim': self.vector_dim,
                'count': self.count(),
                'memories': []
            }

            for memory_id, meta in self.id_to_metadata.items():
                # âš¡ è·³è¿‡å·²åˆ é™¤çš„è®°å¿†
                if meta.get('deleted', False):
                    continue

                backup_data['memories'].append({
                    'id': memory_id,
                    'content': meta['content'],
                    'metadata': meta['metadata'],
                    'timestamp': meta['timestamp'],
                    'importance': meta['importance']
                })
            
            with open(self.backup_file, 'w', encoding='utf-8') as f:
                json.dump(backup_data, f, ensure_ascii=False, indent=2)
            
            self.logger.debug(f"ğŸ’¾ [JSONå¤‡ä»½] å·²å¤‡ä»½ {len(backup_data['memories'])} æ¡è®°å¿†")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ [JSONå¤‡ä»½] å¤‡ä»½å¤±è´¥: {e}")
    
    def force_rebuild(self):
        """å¼ºåˆ¶é‡å»ºç´¢å¼•ï¼ˆæ¸…ç†æ‰€æœ‰å·²åˆ é™¤çš„è®°å¿†ï¼‰

        âš¡ Requirement 7.2: æä¾›æ‰‹åŠ¨è§¦å‘é‡å»ºçš„æ¥å£
        """
        if self._pending_deletes > 0:
            self.logger.info(f"ğŸ”„ [FAISS] æ‰‹åŠ¨è§¦å‘é‡å»ºï¼Œæ¸…ç† {self._pending_deletes} æ¡å·²åˆ é™¤è®°å¿†")
            self._rebuild_index()
            self._pending_deletes = 0
        else:
            self.logger.info("â„¹ï¸ [FAISS] æ— éœ€é‡å»ºï¼Œæ²¡æœ‰å¾…åˆ é™¤çš„è®°å¿†")

    def close(self):
        """å…³é—­å­˜å‚¨ï¼ˆä¿å­˜æ•°æ®ï¼‰

        âš¡ å…³é—­å‰æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç†
        """
        # å¦‚æœæœ‰å¤§é‡å¾…åˆ é™¤è®°å¿†ï¼Œåœ¨å…³é—­å‰æ¸…ç†
        if self._pending_deletes > 0:
            self.logger.info(f"ğŸ”„ [FAISS] å…³é—­å‰æ¸…ç† {self._pending_deletes} æ¡å·²åˆ é™¤è®°å¿†")
            self._rebuild_index()
            self._pending_deletes = 0
        else:
            self._save_to_disk()

        self.logger.info("âœ… [FAISS] å­˜å‚¨å·²å…³é—­å¹¶ä¿å­˜")

