# -*- coding: utf-8 -*-

"""
å¢å¼ºå‹è®°å¿†ç®¡ç†å™¨ï¼ˆåŸºäº Mem0 æ¦‚å¿µï¼‰
æä¾›å¤šçº§è®°å¿†ã€å‘é‡æ£€ç´¢å’Œæ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†

å‡çº§è¯´æ˜ï¼š
- ä½¿ç”¨ FAISS æ›¿ä»£ ChromaDBï¼ˆæ›´ç¨³å®šï¼ŒWindows å…¼å®¹æ€§å¥½ï¼‰
- FAISS ä¸ºä¸»å­˜å‚¨ï¼ŒJSON ä¸ºå¤‡ä»½å­˜å‚¨
- è¯­ä¹‰æ£€ç´¢æ€§èƒ½æ›´ä¼˜
"""

import json
import numpy as np
from typing import List, Dict, Any, Optional
from pathlib import Path
from datetime import datetime
from collections import deque
from core.logger import get_logger
from core.ai_services import EmbeddingService
from modules.ai_assistant.logic.faiss_memory_store import FaissMemoryStore

# å»¶è¿Ÿè·å– loggerï¼Œé¿å…æ¨¡å—å¯¼å…¥æ—¶å¡ä½
def _get_logger():
    return get_logger(__name__)

logger = None  # å»¶è¿Ÿåˆå§‹åŒ–


# æ³¨æ„ï¼šBGEEmbeddingFunctionForMemory å·²ç§»é™¤ï¼ŒFAISS ä¸éœ€è¦


class MemoryLevel:
    """è®°å¿†çº§åˆ«æšä¸¾"""
    USER = "user"           # ç”¨æˆ·çº§ï¼ˆè·¨ä¼šè¯æŒä¹…åŒ–ï¼‰
    SESSION = "session"     # ä¼šè¯çº§ï¼ˆå½“å‰ä¼šè¯ï¼‰
    CONTEXT = "context"     # ä¸Šä¸‹æ–‡çº§ï¼ˆæœ€è¿‘å‡ è½®ï¼‰


class Memory:
    """è®°å¿†é¡¹"""
    
    def __init__(self, content: str, level: str = MemoryLevel.SESSION, 
                 metadata: Optional[Dict] = None, timestamp: Optional[str] = None):
        self.content = content
        self.level = level
        self.metadata = metadata or {}
        self.timestamp = timestamp or datetime.now().isoformat()
        self.importance = self.metadata.get('importance', 0.5)  # 0-1ï¼Œé‡è¦æ€§è¯„åˆ†


class EnhancedMemoryManager:
    """å¢å¼ºå‹è®°å¿†ç®¡ç†å™¨
    
    å‚è€ƒ Mem0 è®¾è®¡ç†å¿µï¼š
    - å¤šçº§è®°å¿†ï¼ˆç”¨æˆ·/ä¼šè¯/ä¸Šä¸‹æ–‡ï¼‰
    - æ™ºèƒ½æ£€ç´¢å’Œè¿‡æ»¤
    - è®°å¿†é‡è¦æ€§è¯„åˆ†
    - æŒä¹…åŒ–å­˜å‚¨
    """
    
    def __init__(self, user_id: str = "default", storage_dir: Optional[Path] = None, memory_compressor=None,
                 embedding_service: Optional[EmbeddingService] = None, db_client=None,
                 similarity_threshold: float = 0.5, max_memories_per_query: int = 3,
                 batch_delete_threshold: int = 50):
        """åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨

        Args:
            user_id: ç”¨æˆ·IDï¼ˆç”¨äºç”¨æˆ·çº§è®°å¿†ï¼‰
            storage_dir: å­˜å‚¨ç›®å½•ï¼ˆç”¨äºæŒä¹…åŒ–ï¼‰
            memory_compressor: è®°å¿†å‹ç¼©å™¨å®ä¾‹ï¼ˆå¯é€‰ï¼‰
            embedding_service: åµŒå…¥æœåŠ¡å®ä¾‹ï¼ˆç”¨äºå‘é‡åŒ–ï¼‰
            db_client: å·²åºŸå¼ƒï¼ˆå…¼å®¹æ€§ä¿ç•™ï¼ŒFAISS ä¸éœ€è¦æ­¤å‚æ•°ï¼‰
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤ 0.5ï¼Œè¿‡æ»¤ä½ç›¸å…³æ€§è®°å¿†ï¼‰
            max_memories_per_query: æ¯æ¬¡æŸ¥è¯¢è¿”å›çš„æœ€å¤§è®°å¿†æ•°é‡ï¼ˆé»˜è®¤ 3ï¼‰
            batch_delete_threshold: FAISS æ‰¹é‡åˆ é™¤é˜ˆå€¼ï¼ˆé»˜è®¤ 50ï¼‰
        """
        self.user_id = user_id
        self.logger = _get_logger()  # å»¶è¿Ÿè·å– logger

        # âš¡ é…ç½®é€‰é¡¹ï¼ˆRequirement 13.5, 7.1ï¼‰
        self.similarity_threshold = similarity_threshold
        self.max_memories_per_query = max_memories_per_query
        self.batch_delete_threshold = batch_delete_threshold
        self.logger.info(f"[è®°å¿†é…ç½®] ç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}, æœ€å¤§è®°å¿†æ•°: {max_memories_per_query}, FAISSæ‰¹é‡åˆ é™¤é˜ˆå€¼: {batch_delete_threshold}")
        
        # å­˜å‚¨ç›®å½•
        if storage_dir:
            self.storage_dir = Path(storage_dir)
            self.storage_dir.mkdir(parents=True, exist_ok=True)
        else:
            from core.utils.path_utils import PathUtils
            path_utils = PathUtils()
            self.storage_dir = path_utils.get_user_data_dir() / "ai_memory"
            self.storage_dir.mkdir(parents=True, exist_ok=True)
        
        self.memory_file = self.storage_dir / f"{user_id}_memory.json"
        
        # è®°å¿†å­˜å‚¨
        self.user_memories: List[Memory] = []      # ç”¨æˆ·çº§ï¼ˆæŒä¹…åŒ–ï¼‰- JSON å¤‡ä»½
        self.session_memories: List[Memory] = []   # ä¼šè¯çº§ï¼ˆä¸´æ—¶ï¼‰
        self.context_buffer = deque(maxlen=10)     # ä¸Šä¸‹æ–‡ç¼“å†²ï¼ˆæœ€è¿‘10è½®ï¼‰
        self.compressed_summary: Optional[str] = None  # å‹ç¼©åçš„å†å²æ‘˜è¦
        
        # è®°å¿†å‹ç¼©å™¨
        self.memory_compressor = memory_compressor
        
        # å‘é‡æ£€ç´¢æ”¯æŒï¼ˆFAISSï¼‰
        self.embedding_service = embedding_service or EmbeddingService()
        
        # åˆå§‹åŒ– FAISS å‘é‡å­˜å‚¨ï¼ˆä¸»å­˜å‚¨ï¼‰
        self.faiss_store = None
        try:
            self.logger.info("ğŸ”§ [FAISS] æ­£åœ¨åˆå§‹åŒ–å‘é‡å­˜å‚¨...")
            self.faiss_store = FaissMemoryStore(
                storage_dir=self.storage_dir,
                vector_dim=512,  # bge-small-zh-v1.5 ç»´åº¦
                user_id=user_id,
                batch_delete_threshold=self.batch_delete_threshold  # âš¡ Requirement 7.1: ä¼ é€’æ‰¹é‡åˆ é™¤é˜ˆå€¼
            )
            self.logger.info(f"âœ… FAISS å‘é‡å­˜å‚¨å·²å¯ç”¨ï¼ˆç”¨æˆ·: {user_id}ï¼Œè®°å¿†æ•°: {self.faiss_store.count()}ï¼‰")
        except ImportError as e:
            self.logger.warning(f"âš ï¸ FAISS æ¨¡å—æœªå®‰è£…ï¼Œå°†ä½¿ç”¨çº¯ JSON æ¨¡å¼: {e}")
            self.faiss_store = None
        except Exception as e:
            self.logger.error(f"âŒ FAISS åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨çº¯ JSON æ¨¡å¼: {e}", exc_info=True)
            self.faiss_store = None
        
        # åŠ è½½æŒä¹…åŒ–è®°å¿†ï¼ˆJSON å¤‡ä»½ï¼‰
        try:
            self._load_user_memories()
        except Exception as e:
            self.logger.error(f"åŠ è½½ç”¨æˆ·è®°å¿†æ—¶å‡ºé”™ï¼ˆéè‡´å‘½ï¼‰: {e}", exc_info=True)
        
        # è‡ªåŠ¨è¿ç§»ï¼šå¦‚æœ FAISS ä¸ºç©ºä½† JSON æœ‰æ•°æ®ï¼Œè‡ªåŠ¨è¿ç§»
        if self.faiss_store is not None:
            self._auto_migrate_json_to_faiss()
        
        self.logger.info(f"å¢å¼ºå‹è®°å¿†ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼ˆç”¨æˆ·: {user_id}ï¼ŒFAISSå‘é‡æ£€ç´¢: {'å·²å¯ç”¨' if self.faiss_store else 'æœªå¯ç”¨'}ï¼‰")
    
    # _init_memory_collection æ–¹æ³•å·²ç§»é™¤ï¼ˆFAISS ä¸éœ€è¦ï¼‰
    
    def add_memory(self, content: str, level: str = MemoryLevel.SESSION, 
                   metadata: Optional[Dict] = None, auto_evaluate: bool = True):
        """æ·»åŠ è®°å¿†
        
        Args:
            content: è®°å¿†å†…å®¹
            level: è®°å¿†çº§åˆ«
            metadata: å…ƒæ•°æ®ï¼ˆå¦‚ç±»å‹ã€æ ‡ç­¾ç­‰ï¼‰
            auto_evaluate: æ˜¯å¦è‡ªåŠ¨è¯„ä¼°é‡è¦æ€§
        """
        memory = Memory(content, level, metadata)
        
        # è‡ªåŠ¨è¯„ä¼°é‡è¦æ€§ï¼ˆç®€å•è§„åˆ™ï¼Œå¯æ‰©å±•ä¸º AI è¯„ä¼°ï¼‰
        if auto_evaluate:
            memory.importance = self._evaluate_importance(content, metadata)
        
        # æ ¹æ®çº§åˆ«æ·»åŠ åˆ°å¯¹åº”å­˜å‚¨
        if level == MemoryLevel.USER:
            self.user_memories.append(memory)
            self._save_user_memories()  # å¤‡ä»½åˆ° JSONï¼ˆç¾éš¾æ¢å¤ï¼‰
            
            # FAISS å‘é‡å­˜å‚¨ï¼ˆä¸»å­˜å‚¨ï¼‰
            if self.faiss_store is not None:
                try:
                    # ç”Ÿæˆå‘é‡
                    vector = self.embedding_service.encode_text([content], convert_to_numpy=True)
                    
                    if vector is not None:
                        # å­˜å…¥ FAISS
                        self.faiss_store.add(
                            content=content,
                            vector=vector,
                            metadata=metadata,
                            importance=memory.importance
                        )
                        self.logger.debug(f"âœ… [FAISS] è®°å¿†å·²ä¿å­˜: {content[:50]}...")
                    else:
                        self.logger.warning("âš ï¸ [FAISS] å‘é‡ç”Ÿæˆå¤±è´¥ï¼Œå·²è·³è¿‡")
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ [FAISS] å­˜å‚¨å¤±è´¥ï¼ˆéè‡´å‘½ï¼‰: {e}")
                    # FAISS å¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼ŒJSON å·²ä¿å­˜
        
        elif level == MemoryLevel.SESSION:
            self.session_memories.append(memory)
        elif level == MemoryLevel.CONTEXT:
            self.context_buffer.append(memory)
        
        self.logger.debug(f"æ·»åŠ è®°å¿† [{level}]: {content[:50]}... (é‡è¦æ€§: {memory.importance:.2f})")
    
    def add_dialogue(self, user_query, assistant_response: str, 
                    auto_classify: bool = True):
        """æ·»åŠ å¯¹è¯åˆ°è®°å¿†
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢ï¼ˆå­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰
            assistant_response: AI å›å¤
            auto_classify: æ˜¯å¦è‡ªåŠ¨åˆ†ç±»é‡è¦æ€§å¹¶é€‰æ‹©å­˜å‚¨çº§åˆ«
        """
        # å¤„ç†åˆ—è¡¨ç±»å‹çš„ user_query
        if isinstance(user_query, list):
            # æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
            user_query_text = " ".join(str(item.get("text", item)) if isinstance(item, dict) else str(item) for item in user_query)
        else:
            user_query_text = str(user_query)
        
        # æ™ºèƒ½ä¿å­˜ï¼šåªä¿å­˜æœ‰ä»·å€¼çš„ä¿¡æ¯ï¼Œè€Œä¸æ˜¯ç®€å•çš„é—®ç­”
        if auto_classify:
            # åˆ¤æ–­ç”¨æˆ·æŸ¥è¯¢æ˜¯å¦åŒ…å«æœ‰ä»·å€¼çš„ä¿¡æ¯ï¼ˆé™ˆè¿°å¥ã€åå¥½ã€èº«ä»½ç­‰ï¼‰
            if self._contains_valuable_info(user_query_text):
                # ä¿å­˜ç”¨æˆ·æä¾›çš„ä¿¡æ¯åˆ°ç”¨æˆ·çº§è®°å¿†
                metadata_user = {'type': 'user_info', 'tags': ['åå¥½', 'èº«ä»½']}
                self.add_memory(user_query_text, MemoryLevel.USER, metadata_user)
                self.logger.info(f"âœ… [æœ‰ä»·å€¼ä¿¡æ¯] ä¿å­˜åˆ°ç”¨æˆ·çº§è®°å¿†: {user_query_text[:50]}...")
            else:
                # æ™®é€šé—®ç­”ï¼Œåªä¿å­˜åˆ°ä¸Šä¸‹æ–‡ç¼“å†²
                metadata_context = {'type': 'dialogue'}
                dialogue_content = f"Q: {user_query_text[:100]}\nA: {assistant_response[:100]}"
                self.add_memory(dialogue_content, MemoryLevel.CONTEXT, metadata_context)
                self.logger.debug(f"[æ™®é€šå¯¹è¯] ä¿å­˜åˆ°ä¸Šä¸‹æ–‡ç¼“å†²")
        else:
            # ä¸åˆ†ç±»ï¼Œç›´æ¥ä¿å­˜åˆ°ä¸Šä¸‹æ–‡
            metadata_context = {'type': 'dialogue'}
            self.add_memory(f"ç”¨æˆ·: {user_query_text}", MemoryLevel.CONTEXT, metadata_context)
        
        self.logger.info(f"[å¯¹è¯å·²ä¿å­˜] ç”¨æˆ·çº§:{len(self.user_memories)}, ä¼šè¯çº§:{len(self.session_memories)}, ä¸Šä¸‹æ–‡:{len(self.context_buffer)}")
    
    def get_relevant_memories(self, query: str, limit: int = None,
                             min_importance: float = None) -> List[Dict[str, Any]]:
        """è·å–ç›¸å…³è®°å¿†ï¼ˆåŸºäºå‘é‡è¯­ä¹‰æ£€ç´¢ï¼‰

        Args:
            query: æŸ¥è¯¢å†…å®¹
            limit: è¿”å›æ•°é‡é™åˆ¶ï¼ˆé»˜è®¤ä½¿ç”¨ max_memories_per_query é…ç½®ï¼‰
            min_importance: æœ€å°é‡è¦æ€§é˜ˆå€¼ï¼ˆå·²åºŸå¼ƒï¼Œä½¿ç”¨ similarity_thresholdï¼‰

        Returns:
            List[Dict]: ç›¸å…³è®°å¿†åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« {'content': str, 'similarity': float, 'source': str}
        """
        # âš¡ ä½¿ç”¨é…ç½®é€‰é¡¹ï¼ˆRequirement 13.5ï¼‰
        if limit is None:
            limit = self.max_memories_per_query
        if min_importance is None:
            min_importance = 0.3  # ä¿æŒå‘åå…¼å®¹

        self.logger.info(f"[è®°å¿†æ£€ç´¢] æŸ¥è¯¢: '{query[:50]}...', é˜ˆå€¼: {self.similarity_threshold}, æœ€å¤§æ•°é‡: {limit}")

        results = []
        
        # é¢„å¤„ç†æŸ¥è¯¢è¯ï¼ˆç”¨äºå…³é”®è¯æ£€ç´¢ï¼‰
        query_lower = query.lower()
        query_words = set([w for w in query_lower.split() if len(w) > 1])
        
        # 1. ä» FAISS å‘é‡æ£€ç´¢ç”¨æˆ·çº§è®°å¿†ï¼ˆè¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰
        if self.faiss_store is not None and self.faiss_store.count() > 0:
            try:
                self.logger.info("ğŸ”® [FAISS æ£€ç´¢] å¯åŠ¨è¯­ä¹‰æœç´¢...")
                
                # ç”ŸæˆæŸ¥è¯¢å‘é‡
                query_vector = self.embedding_service.encode_text([query], convert_to_numpy=True)
                
                if query_vector is not None:
                    # FAISS æ£€ç´¢
                    faiss_results = self.faiss_store.search(
                        query_vector=query_vector,
                        top_k=limit * 2,
                        min_importance=min_importance
                    )
                    
                    for content, similarity, metadata in faiss_results:
                        results.append((content, similarity, 'faiss_vector'))
                    
                    self.logger.info(f"âœ… [FAISS æ£€ç´¢] æ‰¾åˆ° {len(results)} æ¡è®°å¿†ï¼ˆè¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…ï¼‰")
                else:
                    self.logger.warning("âš ï¸ [FAISS æ£€ç´¢] å‘é‡ç”Ÿæˆå¤±è´¥")
                    
            except Exception as e:
                self.logger.error(f"âŒ [FAISS æ£€ç´¢] æ£€ç´¢å¤±è´¥: {e}")
        else:
            self.logger.info("âš ï¸ [FAISS æ£€ç´¢] å‘é‡å­˜å‚¨æœªå¯ç”¨æˆ–ä¸ºç©ºï¼Œä½¿ç”¨ JSON å¤‡ä»½æ£€ç´¢")
            
            # é™çº§åˆ°å…³é”®è¯åŒ¹é…ï¼ˆFAISS ä¸å¯ç”¨æ—¶ï¼‰
            for memory in self.user_memories:
                if memory.importance < min_importance:
                    continue
                
                content_lower = memory.content.lower()
                matched_words = sum(1 for word in query_words if word in content_lower)
                if matched_words > 0:
                    similarity_score = matched_words / max(len(query_words), 1)
                    results.append((memory.content, similarity_score, 'json_fallback'))
            
            self.logger.info(f"âœ… [JSON å¤‡ä»½æ£€ç´¢] æ‰¾åˆ° {len(results)} æ¡è®°å¿†")
        
        # 2. ä»ä¼šè¯çº§å’Œä¸Šä¸‹æ–‡çº§è®°å¿†ä¸­æ£€ç´¢ï¼ˆå…³é”®è¯åŒ¹é…ä½œä¸ºè¡¥å……ï¼‰
        self.logger.info("ğŸ” [å…³é”®è¯æ£€ç´¢] æ‰«æä¼šè¯çº§å’Œä¸Šä¸‹æ–‡çº§è®°å¿†...")
        
        # ä¼šè¯çº§è®°å¿†
        for memory in self.session_memories:
            if memory.importance < min_importance:
                continue
            
            content_lower = memory.content.lower()
            matches = sum(1 for word in query_words if word in content_lower)
            
            if matches > 0:
                score = matches * 0.5 + memory.importance * 0.3
                results.append((memory.content, score, 'keyword_session'))
        
        # ä¸Šä¸‹æ–‡çº§è®°å¿†ï¼ˆæœ€è¿‘å¯¹è¯ï¼‰
        for memory in list(self.context_buffer):
            content_lower = memory.content.lower()
            matches = sum(1 for word in query_words if word in content_lower)
            
            if matches > 0:
                score = matches * 0.3 + memory.importance * 0.2
                results.append((memory.content, score, 'keyword_context'))
        
        # 3. åˆå¹¶å¹¶æ’åºç»“æœ
        results.sort(key=lambda x: x[1], reverse=True)

        # âš¡ åº”ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤ï¼ˆRequirement 13.1, 13.4ï¼‰
        filtered_results = [
            (content, score, source)
            for content, score, source in results
            if score >= self.similarity_threshold
        ]

        # è°ƒè¯•æ—¥å¿—ï¼ˆåŒ…å«ç›¸ä¼¼åº¦åˆ†æ•° - Requirement 13.3ï¼‰
        self.logger.info(f"[è®°å¿†æ£€ç´¢] å…±æ‰¾åˆ° {len(results)} æ¡è®°å¿†ï¼Œè¿‡æ»¤å {len(filtered_results)} æ¡ï¼ˆé˜ˆå€¼: {self.similarity_threshold}ï¼‰")
        self.logger.info(f"[è®°å¿†è¯„åˆ†] å‰ {min(5, len(filtered_results))} æ¡è®°å¿†:")
        for i, (content, score, source) in enumerate(filtered_results[:5], 1):
            self.logger.info(f"  {i}. [{source}] ç›¸ä¼¼åº¦:{score:.3f} | {content[:60]}...")

        # âš¡ é™åˆ¶è¿”å›æ•°é‡ï¼ˆRequirement 13.2ï¼‰
        top_results = filtered_results[:limit]

        # âš¡ è¿”å›åŒ…å«ç›¸ä¼¼åº¦åˆ†æ•°çš„ç»“æœï¼ˆRequirement 13.3ï¼‰
        return [
            {
                'content': content,
                'similarity': round(score, 3),
                'source': source
            }
            for content, score, source in top_results
        ]
    
    def get_recent_context(self, limit: int = 5) -> str:
        """è·å–æœ€è¿‘çš„ä¸Šä¸‹æ–‡ï¼ˆæ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²ï¼‰
        
        Args:
            limit: è·å–æ•°é‡
            
        Returns:
            str: æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«å‹ç¼©æ‘˜è¦ï¼‰
        """
        formatted = []
        
        # å¦‚æœæœ‰å‹ç¼©æ‘˜è¦ï¼Œå…ˆæ·»åŠ 
        if self.compressed_summary:
            formatted.append(self.compressed_summary)
        
        # æ·»åŠ æœ€è¿‘çš„åŸå§‹å¯¹è¯
        recent = list(self.context_buffer)[-limit:]
        if recent:
            formatted.append("[æœ€è¿‘å¯¹è¯ä¸Šä¸‹æ–‡]")
            for memory in recent:
                formatted.append(memory.content)
        
        return "\n".join(formatted) if formatted else ""
    
    def get_user_identity(self) -> str:
        """è·å–ç”¨æˆ·èº«ä»½ä¿¡æ¯ï¼ˆåº”è¯¥èå…¥AIè§’è‰²è®¾å®šçš„è®°å¿†ï¼‰

        ä»ç”¨æˆ·çº§è®°å¿†ä¸­æå–æ‰€æœ‰é‡è¦çš„ç”¨æˆ·ç›¸å…³ä¿¡æ¯

        âš¡ ä¼˜åŒ–ï¼šæ™ºèƒ½è¿‡æ»¤å†²çªçš„èº«ä»½è®¾å®šï¼Œåªä¿ç•™æœ€æ–°çš„

        Returns:
            str: ç”¨æˆ·èº«ä»½ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        """
        if not self.user_memories:
            return ""

        # âš ï¸ ä¿®å¤å¤±å¿†é—®é¢˜ï¼šåªè¿”å›"é™ˆè¿°å¥"ï¼ˆç­”æ¡ˆï¼‰ï¼Œè¿‡æ»¤æ‰"ç–‘é—®å¥"ï¼ˆç”¨æˆ·çš„æé—®ï¼‰
        important_memories = []

        # ç–‘é—®è¯åˆ—è¡¨ï¼ˆç”¨äºè¿‡æ»¤é—®é¢˜ï¼‰
        question_keywords = ['ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'å“ª', 'å—', 'å‘¢', '?', 'ï¼Ÿ']

        # âš¡ AIèº«ä»½è®¾å®šå…³é”®è¯ï¼ˆç”¨äºè¯†åˆ«AIè§’è‰²å˜æ›´ï¼Œä¸åŒ…æ‹¬ç”¨æˆ·èº«ä»½ï¼‰
        # å¿…é¡»åŒæ—¶åŒ…å«å¤šä¸ªå…³é”®è¯æ‰ç®—æ˜¯AIèº«ä»½è®¾å®š
        ai_identity_change_patterns = [
            ['ä»ç°åœ¨å¼€å§‹', 'ä½ æ˜¯'],  # "ä»ç°åœ¨å¼€å§‹ï¼Œä½ æ˜¯..."
            ['ä»ç°åœ¨å¼€å§‹', 'ä½ ä¸æ˜¯'],  # "ä»ç°åœ¨å¼€å§‹ï¼Œä½ ä¸æ˜¯..."
            ['ä½ æ˜¯', 'è§’è‰²'],  # "ä½ æ˜¯XXè§’è‰²"
            ['ä½ æ˜¯', 'äººè®¾'],  # "ä½ æ˜¯XXäººè®¾"
            ['æ‰®æ¼”', 'è§’è‰²'],  # "æ‰®æ¼”XXè§’è‰²"
        ]

        # âš¡ ç”¨æˆ·èº«ä»½å…³é”®è¯ï¼ˆç”¨äºè¯†åˆ«ç”¨æˆ·è‡ªå·±çš„èº«ä»½ä¿¡æ¯ï¼‰
        user_identity_keywords = ['æˆ‘æ˜¯', 'æˆ‘å«', 'æˆ‘çš„åå­—', 'æˆ‘çš„èŒä¸š', 'æˆ‘åš']

        for memory in self.user_memories:
            content = memory.content

            # åªä¿ç•™é«˜é‡è¦æ€§çš„è®°å¿†ï¼ˆ>0.7ï¼‰æˆ–æ˜ç¡®çš„"ç”¨æˆ·ç›¸å…³ä¿¡æ¯"
            is_user_info = content.startswith("ç”¨æˆ·ç›¸å…³ä¿¡æ¯:") or content.startswith("ç”¨æˆ·åå¥½:")
            is_important = memory.importance > 0.7

            if not (is_user_info or is_important):
                continue

            # âš ï¸ å…³é”®ä¿®å¤ï¼šè¿‡æ»¤æ‰ç–‘é—®å¥ï¼ˆç”¨æˆ·çš„æé—®ï¼‰
            # æå–æ ¸å¿ƒå†…å®¹
            core_content = content.replace("ç”¨æˆ·ç›¸å…³ä¿¡æ¯:", "").replace("ç”¨æˆ·åå¥½:", "").strip()

            # å¦‚æœåŒ…å«ç–‘é—®è¯ï¼Œè¯´æ˜è¿™æ˜¯ç”¨æˆ·çš„é—®é¢˜ï¼Œä¸æ˜¯ç­”æ¡ˆï¼Œè·³è¿‡
            is_question = any(keyword in core_content for keyword in question_keywords)

            # âœ… åªä¿ç•™é™ˆè¿°å¥ï¼ˆç­”æ¡ˆï¼‰ï¼Œæ¯”å¦‚"æˆ‘å–œæ¬¢èƒ¡æ¡ƒ"ã€"æˆ‘æ˜¯æ—…è¡Œè€…"
            if not is_question:
                # âš¡ ä¼˜åŒ–ï¼šåŒºåˆ†AIèº«ä»½è®¾å®šå’Œç”¨æˆ·èº«ä»½ä¿¡æ¯
                is_ai_identity_change = False
                for pattern in ai_identity_change_patterns:
                    if all(keyword in core_content for keyword in pattern):
                        is_ai_identity_change = True
                        break

                # âš¡ æ–°å¢ï¼šè¯†åˆ«ç”¨æˆ·èº«ä»½ä¿¡æ¯
                is_user_identity = any(keyword in core_content for keyword in user_identity_keywords)

                important_memories.append((memory, core_content, is_ai_identity_change, is_user_identity))

        if not important_memories:
            return ""

        # âš¡ ä¼˜åŒ–ï¼šåˆ†ç±»è®°å¿† - AIèº«ä»½è®¾å®šã€ç”¨æˆ·èº«ä»½ä¿¡æ¯ã€å…¶ä»–è®°å¿†
        ai_identity_memories = [(m, c) for m, c, is_ai_id, is_user_id in important_memories if is_ai_id]
        user_identity_memories = [(m, c) for m, c, is_ai_id, is_user_id in important_memories if is_user_id and not is_ai_id]
        other_memories = [(m, c) for m, c, is_ai_id, is_user_id in important_memories if not is_ai_id and not is_user_id]

        # âš¡ å…³é”®ä¿®å¤ï¼šåˆ†åˆ«å¤„ç†AIèº«ä»½å’Œç”¨æˆ·èº«ä»½
        filtered_memories = []

        # 1. AIèº«ä»½è®¾å®šï¼šå¦‚æœæœ‰å¤šæ¡ï¼Œåªä¿ç•™æœ€æ–°çš„ä¸€æ¡
        if ai_identity_memories:
            ai_identity_memories.sort(key=lambda x: x[0].timestamp)
            latest_ai_identity = ai_identity_memories[-1]  # æœ€æ–°çš„AIèº«ä»½è®¾å®š
            filtered_memories.append(latest_ai_identity)

        # 2. ç”¨æˆ·èº«ä»½ä¿¡æ¯ï¼šä¿ç•™æ‰€æœ‰ï¼ˆç”¨æˆ·å¯èƒ½æœ‰å¤šä¸ªèº«ä»½å±æ€§ï¼‰
        if user_identity_memories:
            # æŒ‰æ—¶é—´æ’åºï¼Œä¿ç•™æœ€æ–°çš„å‡ æ¡
            user_identity_memories.sort(key=lambda x: x[0].timestamp)
            # æœ€å¤šä¿ç•™5æ¡ç”¨æˆ·èº«ä»½ä¿¡æ¯
            filtered_memories.extend(user_identity_memories[-5:])

        # 3. å…¶ä»–é‡è¦è®°å¿†ï¼šå¦‚æœæ²¡æœ‰AIèº«ä»½è®¾å®šï¼Œæ‰æ·»åŠ å…¶ä»–è®°å¿†
        if not ai_identity_memories and other_memories:
            filtered_memories.extend(other_memories)

        # æŒ‰é‡è¦æ€§æ’åºï¼ˆé‡è¦æ€§é«˜çš„åœ¨åï¼‰
        filtered_memories.sort(key=lambda m: m[0].importance)

        # å»é‡ï¼šå¦‚æœå¤šæ¡è®°å¿†å†…å®¹ç›¸ä¼¼ï¼Œåªä¿ç•™é‡è¦æ€§æœ€é«˜çš„
        unique_memories = []
        seen_contents = set()

        for memory, core_content in filtered_memories:
            # ç®€å•å»é‡ï¼šåªä¿ç•™å‰30ä¸ªå­—ç¬¦è¿›è¡Œæ¯”è¾ƒ
            content_key = core_content[:30]

            if content_key not in seen_contents:
                unique_memories.append(memory.content)
                seen_contents.add(content_key)

        # é™åˆ¶æ•°é‡ï¼šæœ€å¤šè¿”å›10æ¡æœ€é‡è¦çš„è®°å¿†ï¼ˆé¿å…Tokenè¿‡å¤šï¼‰
        if len(unique_memories) > 10:
            unique_memories = unique_memories[-10:]

        # ç»„åˆæ‰€æœ‰è®°å¿†ï¼Œç”¨æ¢è¡Œåˆ†éš”
        result = "\n".join(unique_memories)

        # Debug log
        if hasattr(self, 'logger') and self.logger:
            ai_identity_count = len([m for m, c, is_ai_id, is_user_id in important_memories if is_ai_id])
            user_identity_count = len([m for m, c, is_ai_id, is_user_id in important_memories if is_user_id])
            self.logger.info(f"[get_user_identity] è¿”å› {len(unique_memories)} æ¡è®°å¿†ï¼ˆAIèº«ä»½: {ai_identity_count} æ¡ï¼Œç”¨æˆ·èº«ä»½: {user_identity_count} æ¡ï¼‰")

        return result
    
    def get_user_profile(self) -> str:
        """è·å–ç”¨æˆ·ç”»åƒï¼ˆä»ç”¨æˆ·çº§è®°å¿†ä¸­æå–ï¼Œæ’é™¤èº«ä»½ä¿¡æ¯ï¼‰
        
        Returns:
            str: ç”¨æˆ·ç”»åƒä¿¡æ¯
        """
        if not self.user_memories:
            return ""
        
        # æå–é«˜é‡è¦æ€§è®°å¿†ï¼Œä½†æ’é™¤èº«ä»½ç›¸å…³çš„ï¼ˆé¿å…é‡å¤ï¼‰
        identity_keywords = ['çŒ«å¨˜', 'èº«ä»½', 'æˆ‘æ˜¯', 'å«æˆ‘', 'è§’è‰²', 'äººè®¾', 'å–µ']
        important_memories = [
            m for m in self.user_memories 
            if m.importance > 0.7 and not any(keyword in m.content.lower() for keyword in identity_keywords)
        ]
        
        if not important_memories:
            return ""
        
        profile = ["[ç”¨æˆ·ä¹ æƒ¯å’Œåå¥½]"]
        for memory in important_memories[-5:]:  # æœ€è¿‘5æ¡é‡è¦è®°å¿†
            profile.append(f"- {memory.content}")
        
        return "\n".join(profile)
    
    def compress_old_context(self, conversation_history: List[Dict[str, str]]) -> bool:
        """å‹ç¼©æ—§å¯¹è¯å†å²ä¸ºæ‘˜è¦
        
        å½“å¯¹è¯å†å²è¿‡é•¿æ—¶ï¼Œè‡ªåŠ¨è§¦å‘å‹ç¼©ï¼Œå°†æ—§æ¶ˆæ¯å‹ç¼©ä¸ºæ‘˜è¦
        
        Args:
            conversation_history: å®Œæ•´çš„å¯¹è¯å†å²åˆ—è¡¨
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸå‹ç¼©
        """
        if not self.memory_compressor:
            self.logger.warning("è®°å¿†å‹ç¼©å™¨æœªè®¾ç½®ï¼Œæ— æ³•å‹ç¼©")
            return False
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©
        if not self.memory_compressor.should_compress(len(conversation_history)):
            return False
        
        try:
            # è·å–éœ€è¦å‹ç¼©çš„æ—§æ¶ˆæ¯ï¼ˆä¿ç•™æœ€è¿‘çš„å‡ æ¡ï¼‰
            keep_recent = self.memory_compressor.keep_recent
            old_messages = conversation_history[:-keep_recent] if len(conversation_history) > keep_recent else []
            
            if not old_messages:
                return False
            
            # ç”Ÿæˆå‹ç¼©æ‘˜è¦
            summary = self.memory_compressor.compress_history(old_messages)
            
            if summary:
                self.compressed_summary = summary
                self.logger.info(f"âœ… æˆåŠŸå‹ç¼© {len(old_messages)} æ¡å†å²æ¶ˆæ¯")
                return True
            
            return False
        
        except Exception as e:
            self.logger.error(f"å‹ç¼©å†å²æ—¶å‡ºé”™: {e}", exc_info=True)
            return False
    
    def clear_session(self):
        """æ¸…ç©ºä¼šè¯çº§è®°å¿†"""
        self.session_memories.clear()
        self.context_buffer.clear()
        self.compressed_summary = None  # æ¸…ç©ºå‹ç¼©æ‘˜è¦
        self.logger.info("ä¼šè¯è®°å¿†å·²æ¸…ç©º")
    
    def _evaluate_importance(self, content: str, metadata: Optional[Dict] = None) -> float:
        """è¯„ä¼°è®°å¿†é‡è¦æ€§ï¼ˆç®€å•è§„åˆ™ï¼Œå¯æ‰©å±•ä¸º AI è¯„åˆ†ï¼‰
        
        Args:
            content: å†…å®¹
            metadata: å…ƒæ•°æ®
            
        Returns:
            float: é‡è¦æ€§è¯„åˆ† (0-1)
        """
        score = 0.5  # é»˜è®¤ä¸­ç­‰é‡è¦æ€§
        
        # è§„åˆ™1ï¼šåŒ…å«ç‰¹å®šå…³é”®è¯æå‡é‡è¦æ€§
        important_keywords = ['é”™è¯¯', 'é…ç½®', 'è·¯å¾„', 'æ–‡ä»¶', 'èµ„äº§', 'è®¾ç½®', 'é—®é¢˜']
        content_lower = content.lower()
        matches = sum(1 for keyword in important_keywords if keyword in content_lower)
        score += matches * 0.1
        
        # è§„åˆ™2ï¼šå†…å®¹é•¿åº¦ï¼ˆè¿‡é•¿æˆ–è¿‡çŸ­é™ä½é‡è¦æ€§ï¼‰
        if 20 < len(content) < 200:
            score += 0.1
        
        # è§„åˆ™3ï¼šå…ƒæ•°æ®æ ‡ç­¾
        if metadata and 'tags' in metadata:
            if 'é‡è¦' in str(metadata['tags']):
                score += 0.2
        
        return min(1.0, max(0.0, score))
    
    def _is_important_query(self, query) -> bool:
        """åˆ¤æ–­æŸ¥è¯¢æ˜¯å¦é‡è¦ï¼ˆå€¼å¾—é•¿æœŸä¿å­˜ï¼‰
        
        Args:
            query: æŸ¥è¯¢å†…å®¹ï¼ˆå­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰
            
        Returns:
            bool: æ˜¯å¦é‡è¦
        """
        # å¤„ç†åˆ—è¡¨ç±»å‹ï¼ˆå¤šéƒ¨åˆ†æ¶ˆæ¯ï¼‰
        if isinstance(query, list):
            # æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
            query_text = " ".join(str(item.get("text", item)) if isinstance(item, dict) else str(item) for item in query)
        else:
            query_text = str(query)
        
        # åŒ…å«ç‰¹å®šå…³é”®è¯çš„æŸ¥è¯¢è§†ä¸ºé‡è¦
        important_indicators = ['æ€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'é…ç½®', 'è®¾ç½®', 'é—®é¢˜', 'é”™è¯¯']
        query_lower = query_text.lower()
        
        return any(indicator in query_lower for indicator in important_indicators)
    
    def _contains_valuable_info(self, text: str) -> bool:
        """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦åŒ…å«æœ‰ä»·å€¼çš„ä¿¡æ¯ï¼ˆé™ˆè¿°å¥ã€åå¥½ã€èº«ä»½ç­‰ï¼‰

        Args:
            text: ç”¨æˆ·è¾“å…¥æ–‡æœ¬

        Returns:
            bool: æ˜¯å¦åŒ…å«æœ‰ä»·å€¼ä¿¡æ¯
        """
        text_lower = text.lower()

        # å¼ºé—®å¥æ ‡å¿—ï¼šç›´æ¥æ’é™¤
        strong_question_indicators = [
            'ä½ è¿˜è®°å¾—', 'ä½ çŸ¥é“', 'ä½ è§‰å¾—', 'æ˜¯ä¸æ˜¯',
            'èƒ½ä¸èƒ½', 'ä¼šä¸ä¼š', 'æœ‰æ²¡æœ‰'
        ]
        if any(q in text_lower for q in strong_question_indicators):
            return False

        # ä¸€èˆ¬ç–‘é—®å¥æ ‡å¿—
        question_words = ['å—', 'å‘¢', 'ï¼Ÿ', '?', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'å“ª', 'è°']
        is_question = any(word in text_lower for word in question_words)

        # âš¡ ä¼˜åŒ–ï¼šæ·»åŠ èº«ä»½è®¾å®šå…³é”®è¯ï¼ˆç”¨äºè¯†åˆ«"ä»ç°åœ¨å¼€å§‹ï¼Œä½ æ˜¯XX"ç­‰å¥å­ï¼‰
        identity_patterns = [
            'ä»ç°åœ¨å¼€å§‹', 'ä½ æ˜¯', 'ä½ ä¸æ˜¯', 'å«æˆ‘', 'è§’è‰²', 'äººè®¾', 'æ‰®æ¼”'
        ]
        has_identity = any(pattern in text_lower for pattern in identity_patterns)

        # âš¡ æ–°å¢ï¼šå¯¹è¯é£æ ¼è¦æ±‚å…³é”®è¯ï¼ˆç”¨äºè¯†åˆ«"ä½ è¯´è¯åº”è¯¥XX"ç­‰å¥å­ï¼‰
        style_requirement_patterns = [
            'ä½ è¯´è¯', 'ä½ å›ç­”', 'ä½ å›å¤', 'ä½ å¯¹æˆ‘è¯´è¯',
            'åº”è¯¥', 'è¦', 'éœ€è¦', 'å¿…é¡»', 'ä¸è¦', 'åˆ«',
            'æœ‰æ„Ÿæƒ…', 'æ¸©æŸ”', 'å¯çˆ±', 'ä¸¥è‚ƒ', 'ä¸“ä¸š', 'å¹½é»˜',
            'è¯­æ°”', 'é£æ ¼', 'æ–¹å¼', 'æ€åº¦'
        ]
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è‡³å°‘2ä¸ªé£æ ¼è¦æ±‚å…³é”®è¯ï¼ˆé¿å…è¯¯åˆ¤ï¼‰
        style_keyword_count = sum(1 for pattern in style_requirement_patterns if pattern in text_lower)
        has_style_requirement = style_keyword_count >= 2

        # é™ˆè¿°å…³é”®è¯ï¼ˆæ£€æŸ¥æ˜¯å¦åŒ…å«ï¼Œä¸è¦æ±‚å¿…é¡»åœ¨å¼€å¤´ï¼‰
        statement_patterns = [
            'æˆ‘å–œæ¬¢ç©', 'æˆ‘å–œæ¬¢', 'æˆ‘æ˜¯', 'æˆ‘å«', 'æˆ‘åœ¨', 'æˆ‘çš„åå­—',
            'æˆ‘æƒ³', 'æˆ‘è§‰å¾—', 'æˆ‘è®¤ä¸º', 'æˆ‘éœ€è¦', 'æˆ‘æœ‰', 'æˆ‘ç”¨',
            'æ­£åœ¨å¼€å‘', 'æ­£åœ¨åš', 'æ“…é•¿', 'æœ€å–œæ¬¢çš„', 'æˆ‘çš„èŒä¸š',
            'æˆ‘åš', 'æˆ‘ä¼š', 'æˆ‘æ“…é•¿', 'æˆ‘çš„å·¥ä½œ', 'æˆ‘çš„çˆ±å¥½'
        ]
        # âš¡ ä¿®å¤ï¼šæ”¹ä¸ºæ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯ï¼Œè€Œä¸æ˜¯å¿…é¡»ä»¥å…³é”®è¯å¼€å¤´
        # è¿™æ ·å¯ä»¥è¯†åˆ«"ä½ å¥½ï¼Œæˆ‘æ˜¯å¼ ä¸‰"è¿™æ ·çš„å¥å­
        has_statement = any(p in text_lower for p in statement_patterns)

        # âš¡ ä¼˜åŒ–ï¼šå¦‚æœåŒ…å«èº«ä»½è®¾å®šå…³é”®è¯ï¼Œå³ä½¿æ˜¯é—®å¥ä¹Ÿä¿å­˜
        if has_identity and not is_question:
            return True

        # âš¡ æ–°å¢ï¼šå¦‚æœåŒ…å«å¯¹è¯é£æ ¼è¦æ±‚ï¼Œå³ä½¿ä¸æ˜¯é™ˆè¿°å¥ä¹Ÿä¿å­˜
        if has_style_requirement and not is_question:
            return True

        # å¦‚æœæ˜¯é—®å¥ä½†æ²¡æœ‰å¼ºé™ˆè¿°å¼€å¤´ï¼Œæ’é™¤
        if is_question and not has_statement:
            return False

        # å¿…é¡»åŒ…å«æœ‰ä»·å€¼å…³é”®è¯
        return has_statement
    
    def _load_user_memories(self):
        """ä»æ–‡ä»¶åŠ è½½ç”¨æˆ·çº§è®°å¿†"""
        try:
            if self.memory_file.exists():
                with open(self.memory_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                for item in data.get('memories', []):
                    memory = Memory(
                        content=item['content'],
                        level=MemoryLevel.USER,
                        metadata=item.get('metadata', {}),
                        timestamp=item.get('timestamp')
                    )
                    memory.importance = item.get('importance', 0.5)
                    self.user_memories.append(memory)
                
                self.logger.info(f"åŠ è½½äº† {len(self.user_memories)} æ¡ç”¨æˆ·è®°å¿†")
        
        except Exception as e:
            self.logger.error(f"åŠ è½½ç”¨æˆ·è®°å¿†å¤±è´¥: {e}")
    
    def _save_user_memories(self):
        """ä¿å­˜ç”¨æˆ·çº§è®°å¿†åˆ°æ–‡ä»¶"""
        try:
            data = {
                'user_id': self.user_id,
                'updated_at': datetime.now().isoformat(),
                'memories': [
                    {
                        'content': m.content,
                        'importance': m.importance,
                        'metadata': m.metadata,
                        'timestamp': m.timestamp
                    }
                    for m in self.user_memories
                ]
            }
            
            with open(self.memory_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            self.logger.debug(f"ä¿å­˜äº† {len(self.user_memories)} æ¡ç”¨æˆ·è®°å¿†")
        
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç”¨æˆ·è®°å¿†å¤±è´¥: {e}")
    
    def _auto_migrate_json_to_faiss(self):
        """è‡ªåŠ¨è¿ç§» JSON è®°å¿†åˆ° FAISSï¼ˆé¦–æ¬¡å¯åŠ¨æˆ– FAISS ä¸ºç©ºæ—¶ï¼‰"""
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿ç§»
            faiss_count = self.faiss_store.count() if self.faiss_store else 0
            json_count = len(self.user_memories)
            
            # FAISS ä¸ºç©ºä½† JSON æœ‰æ•°æ® -> éœ€è¦è¿ç§»
            if faiss_count == 0 and json_count > 0:
                self.logger.info(f"ğŸ”„ [è‡ªåŠ¨è¿ç§»] æ£€æµ‹åˆ° {json_count} æ¡ JSON è®°å¿†ï¼Œå¼€å§‹è¿ç§»åˆ° FAISS...")
                
                success_count = 0
                for memory in self.user_memories:
                    try:
                        # ç”Ÿæˆå‘é‡
                        vector = self.embedding_service.encode_text([memory.content], convert_to_numpy=True)
                        
                        if vector is not None:
                            # æ·»åŠ åˆ° FAISS
                            self.faiss_store.add(
                                content=memory.content,
                                vector=vector,
                                metadata=memory.metadata,
                                importance=memory.importance
                            )
                            success_count += 1
                        
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ è¿ç§»å•æ¡è®°å¿†å¤±è´¥: {e}")
                        continue
                
                # å¼ºåˆ¶ä¿å­˜
                if success_count > 0:
                    self.faiss_store._save_to_disk()
                    self.logger.info(f"âœ… [è‡ªåŠ¨è¿ç§»] æˆåŠŸè¿ç§» {success_count}/{json_count} æ¡è®°å¿†åˆ° FAISS")
                else:
                    self.logger.warning("âš ï¸ [è‡ªåŠ¨è¿ç§»] æœªèƒ½è¿ç§»ä»»ä½•è®°å¿†")
            
            elif faiss_count > 0:
                self.logger.info(f"âœ… [FAISS] å·²æœ‰ {faiss_count} æ¡è®°å¿†ï¼Œè·³è¿‡è¿ç§»")
            
        except Exception as e:
            self.logger.error(f"âŒ [è‡ªåŠ¨è¿ç§»] è¿ç§»å¤±è´¥: {e}", exc_info=True)

